{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AskScience_posts = pd.read_json('jsons/r_AskScience_posts.jsonl', lines=True)\n",
    "AskScience_comments = pd.read_json('jsons/r_AskScience_comments.jsonl', lines=True)\n",
    "Medicine_posts = pd.read_json('jsons/r_Medicine_posts.jsonl', lines=True)\n",
    "Medicine_comments = pd.read_json('jsons/r_Medicine_comments.jsonl', lines=True)\n",
    "Psychology_posts = pd.read_json('jsons/r_Psychology_posts.jsonl', lines=True)\n",
    "Psychology_comments = pd.read_json('jsons/r_Psychology_comments.jsonl', lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing r_AskAcademia_comments.jsonl...\n",
      "  Added 185066 rows from r_AskAcademia_comments.jsonl\n",
      "Processing r_AskAcademia_posts.jsonl...\n",
      "  Added 18281 rows from r_AskAcademia_posts.jsonl\n",
      "Processing r_AskScience_comments.jsonl...\n",
      "  Added 130940 rows from r_AskScience_comments.jsonl\n",
      "Processing r_AskScience_posts.jsonl...\n",
      "  Added 95440 rows from r_AskScience_posts.jsonl\n",
      "Processing r_Medicine_comments.jsonl...\n",
      "  Added 297694 rows from r_Medicine_comments.jsonl\n",
      "Processing r_Medicine_posts.jsonl...\n",
      "  Added 19474 rows from r_Medicine_posts.jsonl\n",
      "Processing r_Psychology_comments.jsonl...\n",
      "  Added 160289 rows from r_Psychology_comments.jsonl\n",
      "Processing r_Psychology_posts.jsonl...\n",
      "  Added 12277 rows from r_Psychology_posts.jsonl\n",
      "Processing r_research_posts.jsonl...\n",
      "  Added 7681 rows from r_research_posts.jsonl\n",
      "Processing r_science_comments.jsonl...\n",
      "  Added 1238728 rows from r_science_comments.jsonl\n",
      "Processing r_science_posts.jsonl...\n",
      "  Added 24984 rows from r_science_posts.jsonl\n",
      "Successfully created merged_posts.csv with 178137 rows\n",
      "Successfully created merged_comments.csv with 2012717 rows\n",
      "Merged posts shape: (178137, 119)\n",
      "Merged comments shape: (2012717, 74)\n",
      "\n",
      "Created DataFrames:\n",
      "- AskAcademia_comments\n",
      "- AskAcademia_posts\n",
      "- AskScience_comments\n",
      "- AskScience_posts\n",
      "- Medicine_comments\n",
      "- Medicine_posts\n",
      "- Psychology_comments\n",
      "- Psychology_posts\n",
      "- research_posts\n",
      "- science_comments\n",
      "- science_posts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Path to the folder containing JSONL files\n",
    "json_folder = 'jsons'\n",
    "\n",
    "# Lists to store DataFrames\n",
    "post_dfs = []\n",
    "comment_dfs = []\n",
    "\n",
    "# Dictionary to store DataFrames by subreddit for reference\n",
    "dfs_by_subreddit = {}\n",
    "\n",
    "# Process each file in the folder\n",
    "for filename in os.listdir(json_folder):\n",
    "    if filename.endswith('.jsonl'):\n",
    "        file_path = os.path.join(json_folder, filename)\n",
    "        \n",
    "        # Extract subreddit name and type (posts or comments) using regex\n",
    "        match = re.match(r'r_(\\w+)_(posts|comments)\\.jsonl', filename)\n",
    "        \n",
    "        if match:\n",
    "            subreddit = match.group(1)\n",
    "            content_type = match.group(2)\n",
    "            \n",
    "            print(f\"Processing {filename}...\")\n",
    "            \n",
    "            try:\n",
    "                # Read JSONL directly with pandas\n",
    "                df = pd.read_json(file_path, lines=True)\n",
    "                \n",
    "                # Store in appropriate list based on content type\n",
    "                if content_type == 'posts':\n",
    "                    post_dfs.append(df)\n",
    "                    dfs_by_subreddit[f\"{subreddit}_posts\"] = df\n",
    "                elif content_type == 'comments':\n",
    "                    comment_dfs.append(df)\n",
    "                    dfs_by_subreddit[f\"{subreddit}_comments\"] = df\n",
    "                \n",
    "                print(f\"  Added {len(df)} rows from {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {filename}: {str(e)}\")\n",
    "                print(f\"  Trying alternative method...\")\n",
    "                \n",
    "                try:\n",
    "                    # Fallback method with encoding handling\n",
    "                    with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                        df = pd.read_json(f, lines=True)\n",
    "                    \n",
    "                    if content_type == 'posts':\n",
    "                        post_dfs.append(df)\n",
    "                        dfs_by_subreddit[f\"{subreddit}_posts\"] = df\n",
    "                    elif content_type == 'comments':\n",
    "                        comment_dfs.append(df)\n",
    "                        dfs_by_subreddit[f\"{subreddit}_comments\"] = df\n",
    "                    \n",
    "                    print(f\"  Added {len(df)} rows from {filename} using fallback method\")\n",
    "                    \n",
    "                except Exception as e2:\n",
    "                    print(f\"  Failed with fallback method: {str(e2)}\")\n",
    "\n",
    "# Function to merge DataFrames\n",
    "def merge_dataframes(dfs, output_file):\n",
    "    if not dfs:\n",
    "        print(f\"No DataFrames to merge for {output_file}\")\n",
    "        return None\n",
    "    \n",
    "    # Concatenate all DataFrames\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Save the merged DataFrame to CSV\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"Successfully created {output_file} with {len(merged_df)} rows\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Create variables for individual DataFrames that might be referenced later\n",
    "for key, df in dfs_by_subreddit.items():\n",
    "    # This will create variables like AskScience_posts, Medicine_comments, etc.\n",
    "    exec(f\"{key} = df\")\n",
    "\n",
    "# Merge post DataFrames and save to CSV\n",
    "merged_posts = merge_dataframes(post_dfs, 'merged_posts.csv')\n",
    "\n",
    "# Merge comment DataFrames and save to CSV\n",
    "merged_comments = merge_dataframes(comment_dfs, 'merged_comments.csv')\n",
    "\n",
    "# Output summary\n",
    "if merged_posts is not None:\n",
    "    print(\"Merged posts shape:\", merged_posts.shape)\n",
    "if merged_comments is not None:\n",
    "    print(\"Merged comments shape:\", merged_comments.shape)\n",
    "\n",
    "# List all the DataFrames that were created\n",
    "print(\"\\nCreated DataFrames:\")\n",
    "for key in dfs_by_subreddit.keys():\n",
    "    print(f\"- {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
