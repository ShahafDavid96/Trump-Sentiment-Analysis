{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe04c70",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d43decb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 2.1/33.5 MB 9.8 MB/s eta 0:00:04\n",
      "     -------------- ------------------------ 12.1/33.5 MB 29.1 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 17.3/33.5 MB 28.7 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 21.5/33.5 MB 26.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 29.9/33.5 MB 29.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 33.5/33.5 MB 30.0 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a1f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shahaf\\AppData\\Local\\Temp\\ipykernel_23464\\2459449348.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Shahaf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shahaf/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Shahaf/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Shahaf/nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import community as community_louvain\n",
    "import warnings\n",
    "from gensim import similarities  \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load spaCy model for NLP tasks\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "# Set up matplotlib for better visualizations\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3341f78",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# Part 1: Data Loading (Using Existing CSV Files)\n",
    "# =============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f313f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(posts_csv_path, comments_csv_path):\n",
    "    \"\"\"\n",
    "    Load Reddit data from existing CSV files\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    posts_csv_path : str\n",
    "        Path to the CSV file containing post data\n",
    "    comments_csv_path : str\n",
    "        Path to the CSV file containing comment data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    posts_df : pandas.DataFrame\n",
    "        DataFrame containing posts\n",
    "    comments_df : pandas.DataFrame\n",
    "        DataFrame containing comments\n",
    "    \"\"\"\n",
    "    print(f\"Loading posts from {posts_csv_path}...\")\n",
    "    posts_df = pd.read_csv(posts_csv_path)\n",
    "    \n",
    "    print(f\"Loading comments from {comments_csv_path}...\")\n",
    "    comments_df = pd.read_csv(comments_csv_path)\n",
    "    \n",
    "    # Convert timestamp columns to datetime\n",
    "    timestamp_columns = ['created_utc']\n",
    "    for col in timestamp_columns:\n",
    "        if col in posts_df.columns:\n",
    "            posts_df[col] = pd.to_datetime(posts_df[col])\n",
    "        if col in comments_df.columns:\n",
    "            comments_df[col] = pd.to_datetime(comments_df[col])\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    required_post_columns = ['post_id', 'subreddit', 'title', 'body', 'author', 'created_utc', 'score']\n",
    "    required_comment_columns = ['comment_id', 'post_id', 'subreddit', 'body', 'author', 'created_utc', 'score']\n",
    "    \n",
    "    # Check if required columns exist in post DataFrame\n",
    "    missing_post_columns = [col for col in required_post_columns if col not in posts_df.columns]\n",
    "    if missing_post_columns:\n",
    "        # Try to find alternative column names or create placeholder columns\n",
    "        for col in missing_post_columns:\n",
    "            if col == 'post_id' and 'id' in posts_df.columns:\n",
    "                posts_df['post_id'] = posts_df['id']\n",
    "            elif col == 'body' and 'selftext' in posts_df.columns:\n",
    "                posts_df['body'] = posts_df['selftext']\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in posts CSV. Creating empty column.\")\n",
    "                posts_df[col] = None\n",
    "    \n",
    "    # Check if required columns exist in comment DataFrame\n",
    "    missing_comment_columns = [col for col in required_comment_columns if col not in comments_df.columns]\n",
    "    if missing_comment_columns:\n",
    "        # Try to find alternative column names or create placeholder columns\n",
    "        for col in missing_comment_columns:\n",
    "            if col == 'comment_id' and 'id' in comments_df.columns:\n",
    "                comments_df['comment_id'] = comments_df['id']\n",
    "            else:\n",
    "                print(f\"Warning: Column '{col}' not found in comments CSV. Creating empty column.\")\n",
    "                comments_df[col] = None\n",
    "    \n",
    "    # Print data summary\n",
    "    print(f\"\\nLoaded {len(posts_df)} posts from {posts_df['subreddit'].nunique()} subreddits.\")\n",
    "    print(f\"Loaded {len(comments_df)} comments.\")\n",
    "    \n",
    "    # Print the subreddits found in the data\n",
    "    print(\"\\nSubreddits in the dataset:\")\n",
    "    for subreddit, count in posts_df['subreddit'].value_counts().items():\n",
    "        print(f\"  - r/{subreddit}: {count} posts\")\n",
    "    \n",
    "    return posts_df, comments_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ab64f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading posts from merged_posts.csv...\n",
      "Loading comments from merged_comments.csv...\n",
      "Warning: Column 'post_id' not found in comments CSV. Creating empty column.\n",
      "\n",
      "Loaded 150505 posts from 3 subreddits.\n",
      "Loaded 3561862 comments.\n",
      "\n",
      "Subreddits in the dataset:\n",
      "  - r/Conservative: 132987 posts\n",
      "  - r/centrist: 11059 posts\n",
      "  - r/Liberal: 6459 posts\n"
     ]
    }
   ],
   "source": [
    "posts_csv_path = \"merged_posts.csv\" \n",
    "comments_csv_path = \"merged_comments.csv\" \n",
    "posts_df, comments_df = load_data_from_csv(posts_csv_path, comments_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a9bd5c",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# Part 2: Data Preprocessing\n",
    "# ============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f20fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text data for NLP analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Text to preprocess\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Preprocessed text\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Rejoin tokens\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def process_dataframes(posts_df, comments_df):\n",
    "    \"\"\"\n",
    "    Process the post and comment DataFrames to add NLP features\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    posts_df : pandas.DataFrame\n",
    "        DataFrame containing posts\n",
    "    comments_df : pandas.DataFrame\n",
    "        DataFrame containing comments\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    posts_df : pandas.DataFrame\n",
    "        Processed posts DataFrame with only relevant columns\n",
    "    comments_df : pandas.DataFrame\n",
    "        Processed comments DataFrame with only relevant columns\n",
    "    \"\"\"\n",
    "    print(\"Starting preprocessing...\")\n",
    "    \n",
    "    # Define essential columns to keep for posts\n",
    "    posts_essential_cols = [\n",
    "        'id', 'post_id', 'title', 'selftext', 'author', 'subreddit', \n",
    "        'created_utc', 'score', 'num_comments', 'upvote_ratio'\n",
    "    ]\n",
    "    \n",
    "    # Define essential columns to keep for comments\n",
    "    comments_essential_cols = [\n",
    "        'id', 'comment_id', 'body', 'author', 'subreddit', \n",
    "        'created_utc', 'score', 'parent_id', 'post_id'\n",
    "    ]\n",
    "    \n",
    "    # Keep only columns that exist in the dataframes\n",
    "    posts_cols_to_keep = [col for col in posts_essential_cols if col in posts_df.columns]\n",
    "    comments_cols_to_keep = [col for col in comments_essential_cols if col in comments_df.columns]\n",
    "    \n",
    "    # Add any columns that are missing but required\n",
    "    for col in ['post_id', 'subreddit', 'created_utc', 'score']:\n",
    "        if col not in posts_cols_to_keep and col != 'post_id':  # Skip post_id as it might be derived from id\n",
    "            posts_cols_to_keep.append(col)\n",
    "            posts_df[col] = None\n",
    "            print(f\"Added missing required column '{col}' to posts dataframe\")\n",
    "    \n",
    "    for col in ['comment_id', 'post_id', 'subreddit', 'created_utc', 'score']:\n",
    "        if col not in comments_cols_to_keep and col != 'comment_id':  # Skip comment_id as it might be derived from id\n",
    "            comments_cols_to_keep.append(col)\n",
    "            comments_df[col] = None\n",
    "            print(f\"Added missing required column '{col}' to comments dataframe\")\n",
    "    \n",
    "    # Create a copy with only the essential columns\n",
    "    posts_df_slim = posts_df[posts_cols_to_keep].copy()\n",
    "    comments_df_slim = comments_df[comments_cols_to_keep].copy()\n",
    "    \n",
    "    print(f\"Reduced posts dataframe from {len(posts_df.columns)} to {len(posts_df_slim.columns)} columns\")\n",
    "    print(f\"Reduced comments dataframe from {len(comments_df.columns)} to {len(comments_df_slim.columns)} columns\")\n",
    "    \n",
    "    # Make sure post_id is consistently named\n",
    "    if 'id' in posts_df_slim.columns and 'post_id' not in posts_df_slim.columns:\n",
    "        posts_df_slim['post_id'] = posts_df_slim['id']\n",
    "    \n",
    "    if 'id' in comments_df_slim.columns and 'comment_id' not in comments_df_slim.columns:\n",
    "        comments_df_slim['comment_id'] = comments_df_slim['id']\n",
    "    \n",
    "    # Make sure parent_id in comments is properly formatted\n",
    "    if 'parent_id' in comments_df_slim.columns:\n",
    "        # Extract post_id from parent_id if it's in format \"t3_xxxxx\"\n",
    "        comments_df_slim['post_id'] = comments_df_slim['parent_id'].apply(\n",
    "            lambda x: x.split('_')[1] if isinstance(x, str) and x.startswith('t3_') else x\n",
    "        )\n",
    "        # If parent_id starts with 't1_', it's a reply to another comment\n",
    "        comments_df_slim['is_reply_to_comment'] = comments_df_slim['parent_id'].apply(\n",
    "            lambda x: True if isinstance(x, str) and x.startswith('t1_') else False\n",
    "        )\n",
    "    \n",
    "    # Process posts\n",
    "    print(\"Processing posts text...\")\n",
    "    if 'title' in posts_df_slim.columns and 'selftext' in posts_df_slim.columns:\n",
    "        # Process title\n",
    "        tqdm.pandas(desc=\"Processing post titles\")\n",
    "        posts_df_slim['processed_title'] = posts_df_slim['title'].progress_apply(\n",
    "            lambda x: preprocess_text(x) if pd.notna(x) else \"\"\n",
    "        )\n",
    "        \n",
    "        # Process selftext (body)\n",
    "        tqdm.pandas(desc=\"Processing post bodies\")\n",
    "        posts_df_slim['processed_body'] = posts_df_slim['selftext'].progress_apply(\n",
    "            lambda x: preprocess_text(x) if pd.notna(x) else \"\"\n",
    "        )\n",
    "        \n",
    "        # Create combined text field\n",
    "        posts_df_slim['combined_text'] = posts_df_slim['processed_title'] + ' ' + posts_df_slim['processed_body']\n",
    "    \n",
    "    # Process comments\n",
    "    print(\"Processing comments text...\")\n",
    "    if 'body' in comments_df_slim.columns:\n",
    "        tqdm.pandas(desc=\"Processing comments\")\n",
    "        comments_df_slim['processed_body'] = comments_df_slim['body'].progress_apply(\n",
    "            lambda x: preprocess_text(x) if pd.notna(x) else \"\"\n",
    "        )\n",
    "    \n",
    "    # Add sentiment analysis\n",
    "    try:\n",
    "        print(\"Adding sentiment analysis...\")\n",
    "        # For posts\n",
    "        if 'combined_text' in posts_df_slim.columns:\n",
    "            tqdm.pandas(desc=\"Calculating post sentiment\")\n",
    "            posts_df_slim['sentiment'] = posts_df_slim['combined_text'].progress_apply(\n",
    "                lambda x: TextBlob(x).sentiment.polarity if x else 0\n",
    "            )\n",
    "        \n",
    "        # For comments\n",
    "        if 'processed_body' in comments_df_slim.columns:\n",
    "            tqdm.pandas(desc=\"Calculating comment sentiment\")\n",
    "            comments_df_slim['sentiment'] = comments_df_slim['processed_body'].progress_apply(\n",
    "                lambda x: TextBlob(x).sentiment.polarity if x else 0\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error in sentiment analysis: {e}\")\n",
    "        print(\"Continuing without sentiment analysis...\")\n",
    "    \n",
    "    # Add named entity recognition (if spaCy model is available)\n",
    "    try:\n",
    "        print(\"Adding named entity recognition...\")\n",
    "        # Function to safely extract entities\n",
    "        def extract_entities(text):\n",
    "            if not text or not isinstance(text, str):\n",
    "                return []\n",
    "            try:\n",
    "                # Limit text length to avoid memory issues\n",
    "                text = text[:10000]  # Limit to 10k chars to prevent memory issues\n",
    "                doc = nlp(text)\n",
    "                return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "            except Exception as e:\n",
    "                print(f\"Error in entity extraction: {e}\")\n",
    "                return []\n",
    "        \n",
    "        # For posts\n",
    "        if 'combined_text' in posts_df_slim.columns:\n",
    "            tqdm.pandas(desc=\"Extracting post entities\")\n",
    "            posts_df_slim['entities'] = posts_df_slim['combined_text'].progress_apply(extract_entities)\n",
    "        \n",
    "        # For comments - only process a sample to save time\n",
    "        if 'processed_body' in comments_df_slim.columns:\n",
    "            sample_size = min(5000, len(comments_df_slim))\n",
    "            if len(comments_df_slim) > sample_size:\n",
    "                print(f\"Processing entities for a sample of {sample_size} comments...\")\n",
    "                sample_idx = comments_df_slim.sample(sample_size).index\n",
    "                comments_df_slim.loc[sample_idx, 'entities'] = comments_df_slim.loc[sample_idx, 'processed_body'].progress_apply(extract_entities)\n",
    "            else:\n",
    "                tqdm.pandas(desc=\"Extracting comment entities\")\n",
    "                comments_df_slim['entities'] = comments_df_slim['processed_body'].progress_apply(extract_entities)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error in entity recognition: {e}\")\n",
    "        print(\"Continuing without entity recognition...\")\n",
    "    \n",
    "    # Convert timestamps if needed\n",
    "    for df in [posts_df_slim, comments_df_slim]:\n",
    "        if 'created_utc' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['created_utc']):\n",
    "            print(\"Converting timestamps...\")\n",
    "            try:\n",
    "                df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error converting timestamps: {e}\")\n",
    "                print(\"Attempting alternative timestamp conversion...\")\n",
    "                try:\n",
    "                    df['created_utc'] = pd.to_datetime(df['created_utc'], errors='coerce')\n",
    "                except:\n",
    "                    print(\"Failed to convert timestamps. Some analyses may not work correctly.\")\n",
    "    \n",
    "    # Drop any columns that are no longer needed (optional)\n",
    "    # For example, after processing, we could drop the original text columns to save memory\n",
    "    if 'processed_title' in posts_df_slim.columns and 'processed_body' in posts_df_slim.columns:\n",
    "        posts_cols_to_drop = []  # You could add 'title', 'selftext' here if you don't need them anymore\n",
    "        posts_df_slim = posts_df_slim.drop(columns=[col for col in posts_cols_to_drop if col in posts_df_slim.columns])\n",
    "    \n",
    "    if 'processed_body' in comments_df_slim.columns:\n",
    "        comments_cols_to_drop = []  # You could add 'body' here if you don't need it anymore\n",
    "        comments_df_slim = comments_df_slim.drop(columns=[col for col in comments_cols_to_drop if col in comments_df_slim.columns])\n",
    "    \n",
    "    print(\"Preprocessing complete!\")\n",
    "    return posts_df_slim, comments_df_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67174f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Processing and enriching data...\n",
      "Starting preprocessing...\n",
      "Reduced posts dataframe from 126 to 10 columns\n",
      "Reduced comments dataframe from 80 to 9 columns\n",
      "Processing posts text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing post titles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150505/150505 [00:19<00:00, 7589.29it/s]\n",
      "Processing post bodies: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150505/150505 [00:10<00:00, 14786.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing comments text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3561862/3561862 [08:54<00:00, 6661.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding sentiment analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating post sentiment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150505/150505 [00:09<00:00, 15282.64it/s]\n",
      "Calculating comment sentiment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3561862/3561862 [03:37<00:00, 16393.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding named entity recognition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting post entities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150505/150505 [07:53<00:00, 317.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing entities for a sample of 5000 comments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting post entities: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:18<00:00, 275.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete!\n",
      "Saved processed data to processed_posts.csv and processed_comments.csv\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Data Preprocessing\n",
    "processed_posts_path = 'processed_posts.csv'\n",
    "processed_comments_path = 'processed_comments.csv'\n",
    "\n",
    "# Check if processed files already exist\n",
    "import os\n",
    "if os.path.exists(processed_posts_path) and os.path.exists(processed_comments_path):\n",
    "    print(\"\\n2. Loading existing processed data files...\")\n",
    "    try:\n",
    "        posts_df = pd.read_csv(processed_posts_path)\n",
    "        comments_df = pd.read_csv(processed_comments_path)\n",
    "        \n",
    "        # Convert timestamp columns to datetime\n",
    "        if 'created_utc' in posts_df.columns:\n",
    "            posts_df['created_utc'] = pd.to_datetime(posts_df['created_utc'])\n",
    "        if 'created_utc' in comments_df.columns:\n",
    "            comments_df['created_utc'] = pd.to_datetime(comments_df['created_utc'])\n",
    "        \n",
    "        # Convert entities column from string back to list if needed\n",
    "        if 'entities' in posts_df.columns and posts_df['entities'].dtype == 'object':\n",
    "            try:\n",
    "                import ast\n",
    "                posts_df['entities'] = posts_df['entities'].apply(\n",
    "                    lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "                )\n",
    "            except:\n",
    "                print(\"Warning: Could not convert entities column back to list.\")\n",
    "        \n",
    "        if 'entities' in comments_df.columns and comments_df['entities'].dtype == 'object':\n",
    "            try:\n",
    "                import ast\n",
    "                comments_df['entities'] = comments_df['entities'].apply(\n",
    "                    lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "                )\n",
    "            except:\n",
    "                print(\"Warning: Could not convert entities column back to list.\")\n",
    "        \n",
    "        print(f\"Loaded processed data: {len(posts_df)} posts and {len(comments_df)} comments.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading processed files: {e}\")\n",
    "        print(\"Will preprocess data from scratch...\")\n",
    "        posts_df, comments_df = process_dataframes(posts_df, comments_df)\n",
    "        \n",
    "        # Save processed data\n",
    "        posts_df.to_csv(processed_posts_path, index=False)\n",
    "        comments_df.to_csv(processed_comments_path, index=False)\n",
    "        print(f\"Saved processed data to {processed_posts_path} and {processed_comments_path}\")\n",
    "else:\n",
    "    print(\"\\n2. Processing and enriching data...\")\n",
    "    posts_df, comments_df = process_dataframes(posts_df, comments_df)\n",
    "    \n",
    "    # Save processed data\n",
    "    posts_df.to_csv(processed_posts_path, index=False)\n",
    "    comments_df.to_csv(processed_comments_path, index=False)\n",
    "    print(f\"Saved processed data to {processed_posts_path} and {processed_comments_path}\")\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {\n",
    "    'posts_df': posts_df,\n",
    "    'comments_df': comments_df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2505501f",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# Part 3: Topic Modeling to Identify Claims/Stories\n",
    "# ============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5db6a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Create topic model\\nlda_model, corpus, dictionary = create_topic_model(posts_df, num_topics=15, passes=20)\\n\\n# Display topics\\ndisplay_topics(lda_model)\\n\\n# Get topic distributions\\nposts_df = get_topic_distributions(lda_model, corpus, posts_df)\\n\\n# Save model and data\\nlda_model.save('lda_model.model')\\nwith open('lda_corpus.pkl', 'wb') as f:\\n    pickle.dump(corpus, f)\\ndictionary.save('lda_dictionary.dict')\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def create_topic_model(posts_df, num_topics=10, passes=10, min_posts=5):\n",
    "    \"\"\"\n",
    "    Create a topic model using LDA to identify potential claims/stories\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    posts_df : pandas.DataFrame\n",
    "        DataFrame containing processed posts\n",
    "    num_topics : int\n",
    "        Number of topics to identify\n",
    "    passes : int\n",
    "        Number of passes for LDA\n",
    "    min_posts : int\n",
    "        Minimum number of posts required for topic modeling\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lda_model : gensim.models.LdaModel or None\n",
    "        Trained LDA model or None if not enough data\n",
    "    corpus : list\n",
    "        Document-term matrix\n",
    "    dictionary : gensim.corpora.Dictionary\n",
    "        Dictionary mapping words to their IDs\n",
    "    \"\"\"\n",
    "    # Check if there's enough data for topic modeling\n",
    "    if len(posts_df) < min_posts:\n",
    "        print(f\"Warning: Not enough posts for topic modeling. Need at least {min_posts}.\")\n",
    "        return None, [], None\n",
    "    \n",
    "    # Check if the combined_text column exists\n",
    "    if 'combined_text' not in posts_df.columns:\n",
    "        print(\"Error: 'combined_text' column not found in posts DataFrame.\")\n",
    "        if 'processed_title' in posts_df.columns and 'processed_body' in posts_df.columns:\n",
    "            print(\"Creating combined_text from processed_title and processed_body...\")\n",
    "            posts_df['combined_text'] = posts_df['processed_title'].fillna('') + ' ' + posts_df['processed_body'].fillna('')\n",
    "        elif 'title' in posts_df.columns and 'selftext' in posts_df.columns:\n",
    "            print(\"Creating combined_text from raw title and selftext...\")\n",
    "            posts_df['combined_text'] = posts_df['title'].fillna('') + ' ' + posts_df['selftext'].fillna('')\n",
    "        else:\n",
    "            print(\"Error: Cannot create combined_text. No suitable text columns found.\")\n",
    "            return None, [], None\n",
    "    \n",
    "    # Create texts for topic modeling\n",
    "    texts = []\n",
    "    for text in posts_df['combined_text']:\n",
    "        if isinstance(text, str) and len(text.strip()) > 0:\n",
    "            words = text.split()\n",
    "            if len(words) >= 3:  # Only include texts with at least 3 words\n",
    "                texts.append(words)\n",
    "    \n",
    "    # Check if there are enough texts with content\n",
    "    if len(texts) < min_posts:\n",
    "        print(f\"Warning: Not enough posts with meaningful content for topic modeling. Found {len(texts)}, need at least {min_posts}.\")\n",
    "        return None, [], None\n",
    "    \n",
    "    print(f\"Creating topic model with {len(texts)} documents...\")\n",
    "    \n",
    "    # Create a corpus and dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "    # Check if dictionary has terms\n",
    "    if len(dictionary) == 0:\n",
    "        print(\"Error: Dictionary has no terms. Check text preprocessing.\")\n",
    "        return None, [], None\n",
    "    \n",
    "    # Filter out words that appear in less than 2 documents or more than 90% of documents\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.9)\n",
    "    \n",
    "    # Check if dictionary still has terms after filtering\n",
    "    if len(dictionary) == 0:\n",
    "        print(\"Error: Dictionary has no terms after filtering extremes. Relaxing filter criteria...\")\n",
    "        # Try again with more relaxed criteria\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        dictionary.filter_extremes(no_below=1, no_above=1.0)\n",
    "        \n",
    "        if len(dictionary) == 0:\n",
    "            print(\"Error: Still no terms in dictionary after relaxing filters.\")\n",
    "            return None, [], None\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    \n",
    "    # Check if corpus is empty\n",
    "    if not corpus or all(len(doc) == 0 for doc in corpus):\n",
    "        print(\"Error: Empty corpus. No terms found in documents after preprocessing.\")\n",
    "        return None, [], None\n",
    "    \n",
    "    # Train LDA model\n",
    "    try:\n",
    "        lda_model = models.LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=min(num_topics, len(texts) // 2),  # Ensure num_topics is not too large\n",
    "            passes=passes,\n",
    "            alpha='auto',\n",
    "            eta='auto'\n",
    "        )\n",
    "        print(f\"Successfully created topic model with {lda_model.num_topics} topics.\")\n",
    "        return lda_model, corpus, dictionary\n",
    "    except Exception as e:\n",
    "        print(f\"Error training LDA model: {e}\")\n",
    "        return None, corpus, dictionary\n",
    "\n",
    "def get_topic_distributions(lda_model, corpus, posts_df):\n",
    "    \"\"\"\n",
    "    Get topic distributions for each post\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lda_model : gensim.models.LdaModel\n",
    "        Trained LDA model (or None if topic modeling failed)\n",
    "    corpus : list\n",
    "        Document-term matrix\n",
    "    posts_df : pandas.DataFrame\n",
    "        DataFrame containing posts\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    posts_df : pandas.DataFrame\n",
    "        DataFrame with added topic distribution columns\n",
    "    \"\"\"\n",
    "    # If LDA model is None, return the original DataFrame\n",
    "    if lda_model is None or not corpus:\n",
    "        print(\"Warning: No topic model available. Skipping topic distribution.\")\n",
    "        return posts_df\n",
    "    \n",
    "    # Get topic distributions for each document\n",
    "    topic_distributions = []\n",
    "    for bow in corpus:\n",
    "        topics = lda_model.get_document_topics(bow)\n",
    "        topic_dist = [0] * lda_model.num_topics\n",
    "        for topic_id, prob in topics:\n",
    "            topic_dist[topic_id] = prob\n",
    "        topic_distributions.append(topic_dist)\n",
    "    \n",
    "    # Add topic distributions to DataFrame\n",
    "    topic_df = pd.DataFrame(topic_distributions, \n",
    "                           columns=[f'topic_{i}' for i in range(lda_model.num_topics)])\n",
    "    \n",
    "    # If topic_df is empty, return the original DataFrame\n",
    "    if topic_df.empty:\n",
    "        print(\"Warning: No topic distributions calculated. Skipping topic distribution.\")\n",
    "        return posts_df\n",
    "    \n",
    "    # Reset index to ensure alignment\n",
    "    posts_df = posts_df.reset_index(drop=True)\n",
    "    topic_df = topic_df.reset_index(drop=True)\n",
    "    \n",
    "    # Check if the lengths match\n",
    "    if len(posts_df) != len(topic_df):\n",
    "        print(f\"Warning: Length mismatch between posts_df ({len(posts_df)}) and topic_df ({len(topic_df)}).\")\n",
    "        print(\"Cannot merge topic distributions. Returning original DataFrame.\")\n",
    "        return posts_df\n",
    "    \n",
    "    # Concatenate DataFrames\n",
    "    result_df = pd.concat([posts_df, topic_df], axis=1)\n",
    "    \n",
    "    # Add dominant topic column\n",
    "    topic_columns = [f'topic_{i}' for i in range(lda_model.num_topics)]\n",
    "    if all(col in result_df.columns for col in topic_columns):\n",
    "        result_df['dominant_topic'] = result_df[topic_columns].idxmax(axis=1)\n",
    "        print(\"Successfully added topic distributions and dominant_topic column.\")\n",
    "    else:\n",
    "        print(\"Warning: Topic columns not properly added to DataFrame.\")\n",
    "    \n",
    "    return result_df\n",
    "    \n",
    "def display_topics(lda_model, num_words=10):\n",
    "    \"\"\"\n",
    "    Display the top words for each topic\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lda_model : gensim.models.LdaModel\n",
    "        Trained LDA model\n",
    "    num_words : int\n",
    "        Number of top words to display per topic\n",
    "    \"\"\"\n",
    "    for idx, topic in lda_model.print_topics(num_words=num_words):\n",
    "        print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Example usage\n",
    "\"\"\"\n",
    "# Create topic model\n",
    "lda_model, corpus, dictionary = create_topic_model(posts_df, num_topics=15, passes=20)\n",
    "\n",
    "# Display topics\n",
    "display_topics(lda_model)\n",
    "\n",
    "# Get topic distributions\n",
    "posts_df = get_topic_distributions(lda_model, corpus, posts_df)\n",
    "\n",
    "# Save model and data\n",
    "lda_model.save('lda_model.model')\n",
    "with open('lda_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus, f)\n",
    "dictionary.save('lda_dictionary.dict')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39b52a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Creating topic model to identify claims/stories...\n",
      "Creating topic model with 138758 documents...\n",
      "Successfully created topic model with 15 topics.\n",
      "\n",
      "Identified Topics:\n",
      "Topic 0: 0.278*\"removed\" + 0.185*\"trump\" + 0.029*\"say\" + 0.025*\"president\" + 0.022*\"new\" + 0.014*\"call\" + 0.013*\"court\" + 0.013*\"order\" + 0.012*\"administration\" + 0.011*\"donald\"\n",
      "Topic 1: 0.058*\"democrat\" + 0.044*\"republican\" + 0.042*\"vote\" + 0.042*\"election\" + 0.027*\"news\" + 0.025*\"voter\" + 0.022*\"harris\" + 0.018*\"win\" + 0.016*\"kamala\" + 0.016*\"show\"\n",
      "Topic 2: 0.068*\"state\" + 0.035*\"federal\" + 0.026*\"law\" + 0.021*\"bill\" + 0.017*\"school\" + 0.017*\"public\" + 0.015*\"congress\" + 0.013*\"funding\" + 0.013*\"department\" + 0.013*\"blue\"\n",
      "Topic 3: 0.069*\"year\" + 0.046*\"first\" + 0.032*\"policy\" + 0.031*\"next\" + 0.030*\"last\" + 0.026*\"week\" + 0.023*\"history\" + 0.022*\"term\" + 0.022*\"two\" + 0.018*\"month\"\n",
      "Topic 4: 0.042*\"im\" + 0.037*\"right\" + 0.034*\"conservative\" + 0.031*\"liberal\" + 0.030*\"think\" + 0.018*\"feel\" + 0.017*\"left\" + 0.015*\"question\" + 0.015*\"good\" + 0.014*\"thought\"\n",
      "Topic 5: 0.060*\"party\" + 0.037*\"power\" + 0.036*\"democratic\" + 0.022*\"democracy\" + 0.022*\"member\" + 0.020*\"leader\" + 0.019*\"future\" + 0.019*\"project\" + 0.015*\"fighting\" + 0.013*\"step\"\n",
      "Topic 6: 0.044*\"million\" + 0.040*\"family\" + 0.037*\"job\" + 0.035*\"free\" + 0.030*\"speech\" + 0.024*\"home\" + 0.019*\"food\" + 0.017*\"k\" + 0.015*\"freedom\" + 0.015*\"buy\"\n",
      "Topic 7: 0.080*\"u\" + 0.035*\"tariff\" + 0.033*\"american\" + 0.023*\"government\" + 0.021*\"war\" + 0.016*\"ukraine\" + 0.013*\"russia\" + 0.012*\"cut\" + 0.012*\"china\" + 0.012*\"end\"\n",
      "Topic 8: 0.096*\"musk\" + 0.075*\"elon\" + 0.058*\"doge\" + 0.023*\"employee\" + 0.021*\"fire\" + 0.019*\"service\" + 0.019*\"ai\" + 0.019*\"email\" + 0.018*\"check\" + 0.018*\"data\"\n",
      "Topic 9: 0.028*\"people\" + 0.022*\"like\" + 0.016*\"dont\" + 0.015*\"would\" + 0.015*\"get\" + 0.014*\"one\" + 0.014*\"want\" + 0.013*\"time\" + 0.012*\"make\" + 0.012*\"need\"\n",
      "Topic 10: 0.099*\"biden\" + 0.083*\"house\" + 0.074*\"white\" + 0.027*\"vance\" + 0.024*\"ice\" + 0.024*\"border\" + 0.022*\"joe\" + 0.020*\"bidens\" + 0.019*\"admin\" + 0.018*\"jd\"\n",
      "Topic 11: 0.026*\"full\" + 0.022*\"na\" + 0.019*\"rep\" + 0.018*\"press\" + 0.016*\"propaganda\" + 0.016*\"fake\" + 0.015*\"gon\" + 0.014*\"covid\" + 0.012*\"oh\" + 0.012*\"scared\"\n",
      "Topic 12: 0.048*\"protest\" + 0.048*\"tax\" + 0.040*\"canada\" + 0.025*\"student\" + 0.022*\"city\" + 0.017*\"death\" + 0.017*\"health\" + 0.017*\"abortion\" + 0.016*\"university\" + 0.016*\"college\"\n",
      "Topic 13: 0.054*\"woman\" + 0.030*\"tesla\" + 0.029*\"illegal\" + 0.023*\"child\" + 0.021*\"immigrant\" + 0.020*\"dei\" + 0.020*\"trans\" + 0.018*\"men\" + 0.014*\"immigration\" + 0.014*\"crime\"\n",
      "Topic 14: 0.054*\"medium\" + 0.043*\"maga\" + 0.042*\"social\" + 0.031*\"black\" + 0.029*\"fight\" + 0.022*\"message\" + 0.020*\"lie\" + 0.019*\"red\" + 0.016*\"woke\" + 0.015*\"truth\"\n",
      "Warning: Length mismatch between posts_df (150505) and topic_df (138758).\n",
      "Cannot merge topic distributions. Returning original DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Topic Modeling (skip if too few posts)\n",
    "print(\"\\n3. Creating topic model to identify claims/stories...\")\n",
    "lda_model, corpus, dictionary = create_topic_model(posts_df, num_topics=15, passes=20, min_posts=5)\n",
    "\n",
    "# If topic modeling failed, return early with just the processed data\n",
    "if lda_model is None:\n",
    "    print(\"Topic modeling failed. Returning processed data only.\")\n",
    "    results['lda_model'] = None\n",
    "    results['corpus'] = None\n",
    "\n",
    "# Display topics\n",
    "print(\"\\nIdentified Topics:\")\n",
    "for idx, topic in lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Get topic distributions\n",
    "posts_df = get_topic_distributions(lda_model, corpus, posts_df)\n",
    "\n",
    "# Save model and data\n",
    "lda_model.save('lda_model.model')\n",
    "with open('lda_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus, f)\n",
    "dictionary.save('lda_dictionary.dict')\n",
    "\n",
    "# Update results\n",
    "results['posts_df'] = posts_df\n",
    "results['lda_model'] = lda_model\n",
    "results['corpus'] = corpus\n",
    "results['dictionary'] = dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c641ac",
   "metadata": {},
   "source": [
    "# =============================================\n",
    "# Part 4: Claim Identification and Tracking\n",
    "# ============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a597b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Identify corrections\\ncorrections_df = identify_corrections(posts_df, comments_df, claim_spread_df)\\n\\n# Analyze correction propagation\\npropagation_metrics = analyze_correction_propagation(claim_spread_df, corrections_df)\\n\\n# Visualize correction propagation for a specific claim\\nclaim_id = claim_spread_df['claim_id'].iloc[0]  # Just get the first claim for example\\nvisualize_correction_propagation(claim_spread_df, corrections_df, claim_id)\\n\\n# Save correction data\\ncorrections_df.to_csv('corrections.csv', index=False)\\nwith open('propagation_metrics.json', 'w') as f:\\n    json.dump(propagation_metrics, f, default=str)  # default=str to handle datetime objects\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def identify_potential_claims(posts_df, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Identify potential claims or news stories based on topic modeling and keyword detection\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    posts_df : pandas.DataFrame\n",
    "        DataFrame containing posts with topic distributions\n",
    "    threshold : float\n",
    "        Threshold for topic probability to consider a post as strongly related to a topic\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    claims_df : pandas.DataFrame\n",
    "        DataFrame containing identified claims\n",
    "    \"\"\"\n",
    "    # Check if necessary columns from topic modeling exist\n",
    "    topic_columns = [col for col in posts_df.columns if col.startswith('topic_')]\n",
    "    \n",
    "    # If no topic columns or dominant_topic column doesn't exist, we can't identify claims\n",
    "    if not topic_columns or 'dominant_topic' not in posts_df.columns:\n",
    "        print(\"Warning: Topic modeling columns not found. Cannot identify claims.\")\n",
    "        # Return an empty DataFrame with appropriate columns\n",
    "        return pd.DataFrame(columns=['topic_id', 'representative_post_id', 'representative_title',\n",
    "                                     'common_entities', 'post_count', 'subreddits', 'avg_sentiment',\n",
    "                                     'earliest_post', 'latest_post'])\n",
    "    \n",
    "    # Find posts with high topic probability\n",
    "    high_prob_mask = (posts_df[topic_columns] > threshold).any(axis=1)\n",
    "    high_prob_posts = posts_df[high_prob_mask].copy()\n",
    "    \n",
    "    # If no posts have high topic probability, return empty DataFrame\n",
    "    if high_prob_posts.empty:\n",
    "        print(\"No posts with high topic probability found.\")\n",
    "        return pd.DataFrame(columns=['topic_id', 'representative_post_id', 'representative_title',\n",
    "                                     'common_entities', 'post_count', 'subreddits', 'avg_sentiment',\n",
    "                                     'earliest_post', 'latest_post'])\n",
    "    \n",
    "    # Group by dominant topic\n",
    "    claims = []\n",
    "    for topic in high_prob_posts['dominant_topic'].unique():\n",
    "        try:\n",
    "            # Get the topic index number (e.g., extract 5 from 'topic_5')\n",
    "            topic_idx = int(topic.split('_')[1]) if isinstance(topic, str) else topic\n",
    "        except (IndexError, ValueError):\n",
    "            print(f\"Warning: Invalid topic format: {topic}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        topic_posts = high_prob_posts[high_prob_posts['dominant_topic'] == topic].copy()\n",
    "        \n",
    "        # Skip if no posts for this topic\n",
    "        if topic_posts.empty:\n",
    "            continue\n",
    "            \n",
    "        # Make sure required columns exist\n",
    "        if 'post_id' not in topic_posts.columns:\n",
    "            print(f\"Warning: 'post_id' column not found. Skipping topic {topic}.\")\n",
    "            continue\n",
    "            \n",
    "        # Find most representative post for this topic (highest probability)\n",
    "        topic_col = f'topic_{topic_idx}'\n",
    "        if topic_col in topic_posts.columns:\n",
    "            # Find post with highest probability for this topic\n",
    "            try:\n",
    "                representative_post = topic_posts.loc[topic_posts[topic_col].idxmax()]\n",
    "            except:\n",
    "                # If there's an error, just take the first post\n",
    "                print(f\"Warning: Could not find most representative post for topic {topic}. Using first post.\")\n",
    "                representative_post = topic_posts.iloc[0]\n",
    "        else:\n",
    "            # If topic column doesn't exist, just take the first post\n",
    "            print(f\"Warning: Topic column {topic_col} not found. Using first post as representative.\")\n",
    "            representative_post = topic_posts.iloc[0]\n",
    "        \n",
    "        # Extract common entities in this topic\n",
    "        all_entities = []\n",
    "        if 'entities' in topic_posts.columns:\n",
    "            for ent_list in topic_posts['entities']:\n",
    "                if isinstance(ent_list, list):\n",
    "                    all_entities.extend([ent[0] for ent in ent_list if isinstance(ent, tuple) and len(ent) > 0])\n",
    "        \n",
    "        common_entities = Counter(all_entities).most_common(5) if all_entities else []\n",
    "        \n",
    "        # Get subreddits and sentiment\n",
    "        subreddits = topic_posts['subreddit'].unique().tolist() if 'subreddit' in topic_posts.columns else []\n",
    "        avg_sentiment = topic_posts['sentiment'].mean() if 'sentiment' in topic_posts.columns else 0\n",
    "        \n",
    "        # Get time range\n",
    "        if 'created_utc' in topic_posts.columns:\n",
    "            earliest_post = topic_posts['created_utc'].min()\n",
    "            latest_post = topic_posts['created_utc'].max()\n",
    "        else:\n",
    "            earliest_post = None\n",
    "            latest_post = None\n",
    "        \n",
    "        # Get title\n",
    "        representative_title = representative_post.get('title', '') if hasattr(representative_post, 'get') else ''\n",
    "        \n",
    "        claim = {\n",
    "            'topic_id': topic,\n",
    "            'representative_post_id': representative_post.get('post_id', ''),\n",
    "            'representative_title': representative_title,\n",
    "            'common_entities': common_entities,\n",
    "            'post_count': len(topic_posts),\n",
    "            'subreddits': subreddits,\n",
    "            'avg_sentiment': avg_sentiment,\n",
    "            'earliest_post': earliest_post,\n",
    "            'latest_post': latest_post\n",
    "        }\n",
    "        claims.append(claim)\n",
    "    \n",
    "    # If no claims were identified, return empty DataFrame\n",
    "    if not claims:\n",
    "        print(\"No claims identified.\")\n",
    "        return pd.DataFrame(columns=['topic_id', 'representative_post_id', 'representative_title',\n",
    "                                     'common_entities', 'post_count', 'subreddits', 'avg_sentiment',\n",
    "                                     'earliest_post', 'latest_post'])\n",
    "    \n",
    "    claims_df = pd.DataFrame(claims)\n",
    "    return claims_df\n",
    "\n",
    "def build_claim_similarity_matrix(posts_df, claims_df):\n",
    "    \"\"\"\n",
    "    Build a similarity matrix between posts and identified claims\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    posts_df : pandas.DataFrame\n",
    "        DataFrame containing all posts\n",
    "    claims_df : pandas.DataFrame\n",
    "        DataFrame containing identified claims\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    similarity_matrix : numpy.ndarray\n",
    "        Matrix of similarity scores between posts and claims\n",
    "    post_ids : list\n",
    "        List of post IDs corresponding to rows in the similarity matrix\n",
    "    claim_ids : list\n",
    "        List of claim IDs corresponding to columns in the similarity matrix\n",
    "    \"\"\"\n",
    "    # Check if claims_df is empty\n",
    "    if claims_df.empty:\n",
    "        print(\"Warning: No claims found. Cannot build similarity matrix.\")\n",
    "        return np.array([]), [], []\n",
    "    \n",
    "    # Get representative posts for each claim\n",
    "    claim_post_ids = claims_df['representative_post_id'].tolist()\n",
    "    claim_posts = posts_df[posts_df['post_id'].isin(claim_post_ids)]\n",
    "    \n",
    "    # Check if any representative posts were found\n",
    "    if claim_posts.empty:\n",
    "        print(\"Warning: No representative posts found for claims. Cannot build similarity matrix.\")\n",
    "        return np.array([]), [], []\n",
    "    \n",
    "    # Create TF-IDF vectors for all posts and claim posts\n",
    "    # Ensure 'combined_text' column exists in both dataframes\n",
    "    if 'combined_text' not in posts_df.columns:\n",
    "        print(\"Error: 'combined_text' column not found in posts DataFrame.\")\n",
    "        return np.array([]), [], []\n",
    "    \n",
    "    if 'combined_text' not in claim_posts.columns:\n",
    "        print(\"Error: 'combined_text' column not found in representative posts.\")\n",
    "        return np.array([]), [], []\n",
    "    \n",
    "    # Fill NaN values in combined_text to avoid errors\n",
    "    posts_df_text = posts_df['combined_text'].fillna('')\n",
    "    claim_posts_text = claim_posts['combined_text'].fillna('')\n",
    "    \n",
    "    try:\n",
    "        # Create TF-IDF vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        \n",
    "        # Fit and transform all posts to create the vocabulary\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(posts_df_text)\n",
    "        \n",
    "        # Transform claim posts using the same vocabulary\n",
    "        claim_tfidf_matrix = tfidf_vectorizer.transform(claim_posts_text)\n",
    "        \n",
    "        # Calculate cosine similarity between all posts and claim posts\n",
    "        # Use sklearn's cosine_similarity instead of gensim's similarities\n",
    "        similarity_matrix = cosine_similarity(tfidf_matrix, claim_tfidf_matrix)\n",
    "        \n",
    "        # Get post IDs and claim IDs for reference\n",
    "        post_ids = posts_df['post_id'].tolist()\n",
    "        \n",
    "        # Map claim post IDs to claim IDs\n",
    "        post_id_to_claim_id = {}\n",
    "        for _, claim in claims_df.iterrows():\n",
    "            post_id_to_claim_id[claim['representative_post_id']] = claim['topic_id']\n",
    "        \n",
    "        claim_ids = [post_id_to_claim_id.get(pid, None) for pid in claim_post_ids]\n",
    "        \n",
    "        return similarity_matrix, post_ids, claim_ids\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error building similarity matrix: {e}\")\n",
    "        return np.array([]), [], []\n",
    "\n",
    "def track_claim_spread(posts_df, comments_df, similarity_matrix, post_ids, claim_ids, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Track how claims spread across different subreddits over time\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    posts_df : pandas.DataFrame\n",
    "        DataFrame containing all posts\n",
    "    comments_df : pandas.DataFrame\n",
    "        DataFrame containing all comments\n",
    "    similarity_matrix : numpy.ndarray\n",
    "        Matrix of similarity scores between posts and claims\n",
    "    post_ids : list\n",
    "        List of post IDs corresponding to rows in the similarity matrix\n",
    "    claim_ids : list\n",
    "        List of claim IDs corresponding to columns in the similarity matrix\n",
    "    threshold : float\n",
    "        Threshold for similarity score to consider a post as related to a claim\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    claim_spread_df : pandas.DataFrame\n",
    "        DataFrame containing information about claim spread\n",
    "    \"\"\"\n",
    "    # Check if inputs are valid\n",
    "    if similarity_matrix.size == 0 or not post_ids or not claim_ids:\n",
    "        print(\"Warning: Empty similarity matrix or ID lists. Cannot track claim spread.\")\n",
    "        return pd.DataFrame(columns=['claim_id', 'post_id', 'subreddit', 'created_utc', \n",
    "                                     'similarity_score', 'sentiment', 'score', 'num_comments'])\n",
    "    \n",
    "    claim_spread = []\n",
    "    \n",
    "    # Create a mapping from post_id to index\n",
    "    post_id_to_idx = {post_id: idx for idx, post_id in enumerate(post_ids)}\n",
    "    \n",
    "    # For each claim\n",
    "    for claim_idx, claim_id in enumerate(claim_ids):\n",
    "        if claim_idx >= similarity_matrix.shape[1]:\n",
    "            print(f\"Warning: claim_idx {claim_idx} is out of bounds for similarity_matrix with shape {similarity_matrix.shape}\")\n",
    "            continue\n",
    "            \n",
    "        # Find posts that are similar to this claim\n",
    "        similar_posts_mask = similarity_matrix[:, claim_idx] > threshold\n",
    "        similar_post_indices = np.where(similar_posts_mask)[0]\n",
    "        \n",
    "        for idx in similar_post_indices:\n",
    "            if idx >= len(post_ids):\n",
    "                print(f\"Warning: post index {idx} is out of bounds for post_ids with length {len(post_ids)}\")\n",
    "                continue\n",
    "                \n",
    "            post_id = post_ids[idx]\n",
    "            matching_posts = posts_df[posts_df['post_id'] == post_id]\n",
    "            \n",
    "            if matching_posts.empty:\n",
    "                print(f\"Warning: No post found with ID {post_id}\")\n",
    "                continue\n",
    "                \n",
    "            post = matching_posts.iloc[0]\n",
    "            \n",
    "            # Get values with error handling\n",
    "            subreddit = post.get('subreddit', 'unknown')\n",
    "            created_utc = post.get('created_utc', None)\n",
    "            sentiment = post.get('sentiment', 0)\n",
    "            score = post.get('score', 0)\n",
    "            num_comments = post.get('num_comments', 0)\n",
    "            \n",
    "            spread_info = {\n",
    "                'claim_id': claim_id,\n",
    "                'post_id': post_id,\n",
    "                'subreddit': subreddit,\n",
    "                'created_utc': created_utc,\n",
    "                'similarity_score': similarity_matrix[idx, claim_idx],\n",
    "                'sentiment': sentiment,\n",
    "                'score': score,\n",
    "                'num_comments': num_comments\n",
    "            }\n",
    "            claim_spread.append(spread_info)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    if claim_spread:\n",
    "        claim_spread_df = pd.DataFrame(claim_spread)\n",
    "        \n",
    "        # Sort by creation time if possible\n",
    "        if 'created_utc' in claim_spread_df.columns:\n",
    "            claim_spread_df = claim_spread_df.sort_values(['claim_id', 'created_utc'])\n",
    "    else:\n",
    "        # Create empty DataFrame with correct columns\n",
    "        claim_spread_df = pd.DataFrame(columns=['claim_id', 'post_id', 'subreddit', 'created_utc', \n",
    "                                               'similarity_score', 'sentiment', 'score', 'num_comments'])\n",
    "    \n",
    "    return claim_spread_df\n",
    "\n",
    "def analyze_correction_propagation(claim_spread_df, corrections_df):\n",
    "    \"\"\"\n",
    "    Analyze how corrections propagate compared to original claims\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    claim_spread_df : pandas.DataFrame\n",
    "        DataFrame containing information about claim spread\n",
    "    corrections_df : pandas.DataFrame\n",
    "        DataFrame containing identified corrections\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    propagation_metrics : dict\n",
    "        Dictionary containing metrics about correction propagation\n",
    "    \"\"\"\n",
    "    propagation_metrics = {}\n",
    "    \n",
    "    # For each claim\n",
    "    for claim_id in claim_spread_df['claim_id'].unique():\n",
    "        # Get claim spread data\n",
    "        claim_data = claim_spread_df[claim_spread_df['claim_id'] == claim_id]\n",
    "        \n",
    "        # Get corrections for this claim\n",
    "        claim_corrections = corrections_df[corrections_df['claim_id'] == claim_id]\n",
    "        \n",
    "        if claim_corrections.empty:\n",
    "            continue\n",
    "        \n",
    "        # Calculate time metrics\n",
    "        claim_first_appearance = claim_data['created_utc'].min()\n",
    "        \n",
    "        if not claim_corrections.empty:\n",
    "            first_correction = claim_corrections['created_utc'].min()\n",
    "            time_to_first_correction = (first_correction - claim_first_appearance).total_seconds() / 3600  # hours\n",
    "        else:\n",
    "            time_to_first_correction = None\n",
    "        \n",
    "        # Calculate subreddit overlap\n",
    "        claim_subreddits = set(claim_data['subreddit'])\n",
    "        correction_subreddits = set(claim_corrections['subreddit'])\n",
    "        subreddit_overlap = len(claim_subreddits.intersection(correction_subreddits))\n",
    "        subreddit_overlap_ratio = subreddit_overlap / len(claim_subreddits) if claim_subreddits else 0\n",
    "        \n",
    "        # Calculate engagement metrics\n",
    "        avg_claim_score = claim_data['score'].mean()\n",
    "        avg_correction_score = claim_corrections['score'].mean() if not claim_corrections.empty else 0\n",
    "        relative_engagement = avg_correction_score / avg_claim_score if avg_claim_score != 0 else 0\n",
    "        \n",
    "        # Store metrics\n",
    "        propagation_metrics[claim_id] = {\n",
    "            'claim_first_appearance': claim_first_appearance,\n",
    "            'first_correction': first_correction if not claim_corrections.empty else None,\n",
    "            'time_to_first_correction': time_to_first_correction,\n",
    "            'num_claim_posts': len(claim_data),\n",
    "            'num_corrections': len(claim_corrections),\n",
    "            'correction_ratio': len(claim_corrections) / len(claim_data),\n",
    "            'claim_subreddits': claim_subreddits,\n",
    "            'correction_subreddits': correction_subreddits,\n",
    "            'subreddit_overlap': subreddit_overlap,\n",
    "            'subreddit_overlap_ratio': subreddit_overlap_ratio,\n",
    "            'avg_claim_score': avg_claim_score,\n",
    "            'avg_correction_score': avg_correction_score,\n",
    "            'relative_engagement': relative_engagement\n",
    "        }\n",
    "    \n",
    "    return propagation_metrics\n",
    "\n",
    "def visualize_correction_propagation(claim_spread_df, corrections_df, claim_id):\n",
    "    \"\"\"\n",
    "    Visualize how a specific claim and its corrections propagate over time\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    claim_spread_df : pandas.DataFrame\n",
    "        DataFrame containing information about claim spread\n",
    "    corrections_df : pandas.DataFrame\n",
    "        DataFrame containing identified corrections\n",
    "    claim_id : str\n",
    "        ID of the claim to visualize\n",
    "    \"\"\"\n",
    "    # Get data for this claim\n",
    "    claim_data = claim_spread_df[claim_spread_df['claim_id'] == claim_id]\n",
    "    claim_corrections = corrections_df[corrections_df['claim_id'] == claim_id]\n",
    "    \n",
    "    # Create a timeline\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot claim posts\n",
    "    for idx, (_, post) in enumerate(claim_data.iterrows()):\n",
    "        plt.scatter(post['created_utc'], idx % 10, c='red', s=post['score']/10 + 50, alpha=0.7)\n",
    "        plt.text(post['created_utc'], idx % 10 + 0.5, post['subreddit'], fontsize=8)\n",
    "    \n",
    "    # Plot corrections\n",
    "    for idx, (_, correction) in enumerate(claim_corrections.iterrows()):\n",
    "        plt.scatter(correction['created_utc'], (idx % 10) - 15, c='green', s=correction['score']/10 + 50, alpha=0.7)\n",
    "        plt.text(correction['created_utc'], (idx % 10) - 14.5, correction['subreddit'], fontsize=8)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Position (for visualization only)')\n",
    "    plt.title(f'Propagation Timeline for Claim {claim_id}')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.scatter([], [], c='red', s=100, label='Original claim posts')\n",
    "    plt.scatter([], [], c='green', s=100, label='Correction posts/comments')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Format x-axis as dates\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "\"\"\"\n",
    "# Identify corrections\n",
    "corrections_df = identify_corrections(posts_df, comments_df, claim_spread_df)\n",
    "\n",
    "# Analyze correction propagation\n",
    "propagation_metrics = analyze_correction_propagation(claim_spread_df, corrections_df)\n",
    "\n",
    "# Visualize correction propagation for a specific claim\n",
    "claim_id = claim_spread_df['claim_id'].iloc[0]  # Just get the first claim for example\n",
    "visualize_correction_propagation(claim_spread_df, corrections_df, claim_id)\n",
    "\n",
    "# Save correction data\n",
    "corrections_df.to_csv('corrections.csv', index=False)\n",
    "with open('propagation_metrics.json', 'w') as f:\n",
    "    json.dump(propagation_metrics, f, default=str)  # default=str to handle datetime objects\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8465d1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Identifying potential claims...\n",
      "Warning: Topic modeling columns not found. Cannot identify claims.\n",
      "No claims identified. Returning processed data and topic model only.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Claim Identification and Tracking\n",
    "print(\"\\n4. Identifying potential claims...\")\n",
    "claims_df = identify_potential_claims(posts_df, threshold=0.6)\n",
    "# If no claims were identified, return early\n",
    "if claims_df.empty:\n",
    "    print(\"No claims identified. Returning processed data and topic model only.\")\n",
    "    results['claims_df'] = claims_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0167b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>representative_post_id</th>\n",
       "      <th>representative_title</th>\n",
       "      <th>common_entities</th>\n",
       "      <th>post_count</th>\n",
       "      <th>subreddits</th>\n",
       "      <th>avg_sentiment</th>\n",
       "      <th>earliest_post</th>\n",
       "      <th>latest_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [topic_id, representative_post_id, representative_title, common_entities, post_count, subreddits, avg_sentiment, earliest_post, latest_post]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce5fd63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Analyze linguistic differences\\nlinguistic_metrics = analyze_linguistic_differences(posts_df, claim_spread_df)\\n\\n# Compare framing for a specific claim\\nclaim_id = claim_spread_df['claim_id'].iloc[0]  # Just get the first claim for example\\nframing_df = compare_framing_across_communities(posts_df, claim_spread_df, claim_id)\\n\\n# Visualize sentiment comparison\\nvisualize_sentiment_comparison(framing_df)\\n\\n# Save linguistic analysis data\\nwith open('linguistic_metrics.json', 'w') as f:\\n    json.dump(linguistic_metrics, f, default=str)\\nframing_df.to_csv(f'framing_comparison_claim_{claim_id}.csv', index=False)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================\n",
    "# Part 7: Linguistic Analysis\n",
    "# =============================================\n",
    "\n",
    "def analyze_linguistic_differences(posts_df, claim_spread_df):\n",
    "    \"\"\"\n",
    "    Analyze linguistic differences in how the same information is presented in different communities\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    posts_df : pandas.DataFrame\n",
    "        DataFrame containing all posts\n",
    "    claim_spread_df : pandas.DataFrame\n",
    "        DataFrame containing information about claim spread\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    linguistic_metrics : dict\n",
    "        Dictionary containing metrics about linguistic differences\n",
    "    \"\"\"\n",
    "    linguistic_metrics = {}\n",
    "    \n",
    "    # For each claim\n",
    "    for claim_id in claim_spread_df['claim_id'].unique():\n",
    "        # Get claim spread data\n",
    "        claim_data = claim_spread_df[claim_spread_df['claim_id'] == claim_id]\n",
    "        \n",
    "        # Get post data for this claim\n",
    "        claim_posts = posts_df[posts_df['post_id'].isin(claim_data['post_id'])]\n",
    "        \n",
    "        # Group by subreddit\n",
    "        subreddit_metrics = {}\n",
    "        for subreddit, subreddit_posts in claim_posts.groupby('subreddit'):\n",
    "            # Skip if there are too few posts\n",
    "            if len(subreddit_posts) < 3:\n",
    "                continue\n",
    "                \n",
    "            # Calculate linguistic metrics\n",
    "            metrics = {\n",
    "                'num_posts': len(subreddit_posts),\n",
    "                'avg_sentiment': subreddit_posts['sentiment'].mean(),\n",
    "                'sentiment_std': subreddit_posts['sentiment'].std(),\n",
    "                'avg_score': subreddit_posts['score'].mean(),\n",
    "                'common_entities': []\n",
    "            }\n",
    "            \n",
    "            # Calculate word frequencies\n",
    "            all_text = ' '.join(subreddit_posts['combined_text'].tolist())\n",
    "            tokens = word_tokenize(all_text)\n",
    "            freq_dist = nltk.FreqDist(tokens)\n",
    "            metrics['top_words'] = freq_dist.most_common(20)\n",
    "            \n",
    "            # Extract common entities\n",
    "            all_entities = []\n",
    "            for ent_list in subreddit_posts['entities']:\n",
    "                all_entities.extend([ent[0] for ent in ent_list])\n",
    "            metrics['common_entities'] = Counter(all_entities).most_common(10)\n",
    "            \n",
    "            subreddit_metrics[subreddit] = metrics\n",
    "        \n",
    "        linguistic_metrics[claim_id] = subreddit_metrics\n",
    "    \n",
    "    return linguistic_metrics\n",
    "\n",
    "def compare_framing_across_communities(posts_df, claim_spread_df, claim_id):\n",
    "    \"\"\"\n",
    "    Compare how the same information is framed across different communities\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    posts_df : pandas.DataFrame\n",
    "        DataFrame containing all posts\n",
    "    claim_spread_df : pandas.DataFrame\n",
    "        DataFrame containing information about claim spread\n",
    "    claim_id : str\n",
    "        ID of the claim to analyze\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    framing_comparison : pandas.DataFrame\n",
    "        DataFrame containing framing comparison metrics\n",
    "    \"\"\"\n",
    "    # Get claim spread data\n",
    "    claim_data = claim_spread_df[claim_spread_df['claim_id'] == claim_id]\n",
    "    \n",
    "    # Get post data for this claim\n",
    "    claim_posts = posts_df[posts_df['post_id'].isin(claim_data['post_id'])]\n",
    "    \n",
    "    # Prepare framing metrics\n",
    "    framing_data = []\n",
    "    \n",
    "    # Group by subreddit\n",
    "    for subreddit, subreddit_posts in claim_posts.groupby('subreddit'):\n",
    "        # Skip if there are too few posts\n",
    "        if len(subreddit_posts) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Calculate sentiment metrics\n",
    "        sentiment_mean = subreddit_posts['sentiment'].mean()\n",
    "        sentiment_std = subreddit_posts['sentiment'].std()\n",
    "        \n",
    "        # Analyze title framing\n",
    "        titles = ' '.join(subreddit_posts['title'].tolist())\n",
    "        title_doc = nlp(titles)\n",
    "        \n",
    "        # Extract named entities\n",
    "        entities = [ent.text for ent in title_doc.ents]\n",
    "        top_entities = Counter(entities).most_common(5)\n",
    "        \n",
    "        # Extract noun phrases\n",
    "        noun_phrases = [chunk.text for chunk in title_doc.noun_chunks]\n",
    "        top_noun_phrases = Counter(noun_phrases).most_common(5)\n",
    "        \n",
    "        # Check for emotional language\n",
    "        positive_words = sum(1 for token in title_doc if token.text.lower() in ['good', 'great', 'excellent', 'positive', 'success'])\n",
    "        negative_words = sum(1 for token in title_doc if token.text.lower() in ['bad', 'terrible', 'awful', 'negative', 'failure'])\n",
    "        emotional_ratio = (positive_words + negative_words) / len(title_doc) if len(title_doc) > 0 else 0\n",
    "        \n",
    "        # Store metrics\n",
    "        framing = {\n",
    "            'claim_id': claim_id,\n",
    "            'subreddit': subreddit,\n",
    "            'num_posts': len(subreddit_posts),\n",
    "            'avg_sentiment': sentiment_mean,\n",
    "            'sentiment_std': sentiment_std,\n",
    "            'top_entities': top_entities,\n",
    "            'top_noun_phrases': top_noun_phrases,\n",
    "            'positive_words': positive_words,\n",
    "            'negative_words': negative_words,\n",
    "            'emotional_ratio': emotional_ratio\n",
    "        }\n",
    "        framing_data.append(framing)\n",
    "    \n",
    "    framing_df = pd.DataFrame(framing_data)\n",
    "    return framing_df\n",
    "\n",
    "def visualize_sentiment_comparison(framing_df):\n",
    "    \"\"\"\n",
    "    Visualize sentiment comparison across subreddits\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    framing_df : pandas.DataFrame\n",
    "        DataFrame containing framing comparison metrics\n",
    "    \"\"\"\n",
    "    # Sort by average sentiment\n",
    "    framing_df = framing_df.sort_values('avg_sentiment')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Create sentiment bar chart\n",
    "    bars = plt.bar(framing_df['subreddit'], framing_df['avg_sentiment'], \n",
    "                  yerr=framing_df['sentiment_std'],\n",
    "                  color=plt.cm.RdBu(np.interp(framing_df['avg_sentiment'], [-1, 1], [0, 1])))\n",
    "    \n",
    "    # Add labels\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.xlabel('Subreddit')\n",
    "    plt.ylabel('Average Sentiment')\n",
    "    plt.title('Sentiment Comparison Across Political Subreddits')\n",
    "    \n",
    "    # Add post count as text\n",
    "    for idx, row in framing_df.iterrows():\n",
    "        plt.text(idx, row['avg_sentiment'] + (0.1 if row['avg_sentiment'] > 0 else -0.1), \n",
    "                f\"n={row['num_posts']}\", ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "\"\"\"\n",
    "# Analyze linguistic differences\n",
    "linguistic_metrics = analyze_linguistic_differences(posts_df, claim_spread_df)\n",
    "\n",
    "# Compare framing for a specific claim\n",
    "claim_id = claim_spread_df['claim_id'].iloc[0]  # Just get the first claim for example\n",
    "framing_df = compare_framing_across_communities(posts_df, claim_spread_df, claim_id)\n",
    "\n",
    "# Visualize sentiment comparison\n",
    "visualize_sentiment_comparison(framing_df)\n",
    "\n",
    "# Save linguistic analysis data\n",
    "with open('linguistic_metrics.json', 'w') as f:\n",
    "    json.dump(linguistic_metrics, f, default=str)\n",
    "framing_df.to_csv(f'framing_comparison_claim_{claim_id}.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "00499e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Create summary dashboard\\ncreate_summary_dashboard(claim_spread_df, corrections_df, propagation_metrics, network_metrics)\\n\\n# Generate insights report\\ninsights = generate_insights_report(claims_df, claim_spread_df, corrections_df, \\n                                  propagation_metrics, network_metrics, linguistic_metrics)\\n\\n# Save insights\\nwith open(\\'misinformation_insights.json\\', \\'w\\') as f:\\n    json.dump(insights, f, indent=2)\\n\\n# Print key findings\\nprint(\"Key Findings:\")\\nfor idx, finding in enumerate(insights[\\'key_findings\\'], 1):\\n    print(f\"{idx}. {finding}\")\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================\n",
    "# Part 8: Visualize Overall Insights\n",
    "# =============================================\n",
    "\n",
    "def create_summary_dashboard(claim_spread_df, corrections_df, propagation_metrics, network_metrics):\n",
    "    \"\"\"\n",
    "    Create a summary dashboard of key findings\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    claim_spread_df : pandas.DataFrame\n",
    "        DataFrame containing information about claim spread\n",
    "    corrections_df : pandas.DataFrame\n",
    "        DataFrame containing identified corrections\n",
    "    propagation_metrics : dict\n",
    "        Dictionary containing metrics about correction propagation\n",
    "    network_metrics : dict\n",
    "        Dictionary containing network analysis metrics\n",
    "    \"\"\"\n",
    "    # Create a 2x2 grid of subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Claim spread by subreddit\n",
    "    claim_counts = claim_spread_df.groupby('subreddit').size().sort_values(ascending=False)\n",
    "    claim_counts.head(10).plot(kind='bar', ax=axes[0, 0], color='skyblue')\n",
    "    axes[0, 0].set_title('Top 10 Subreddits by Claim Appearance')\n",
    "    axes[0, 0].set_ylabel('Number of Posts')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Correction effectiveness\n",
    "    correction_ratios = []\n",
    "    for claim_id, metrics in propagation_metrics.items():\n",
    "        correction_ratios.append({\n",
    "            'claim_id': claim_id,\n",
    "            'correction_ratio': metrics['correction_ratio'],\n",
    "            'relative_engagement': metrics['relative_engagement'],\n",
    "            'subreddit_overlap_ratio': metrics['subreddit_overlap_ratio']\n",
    "        })\n",
    "    \n",
    "    correction_df = pd.DataFrame(correction_ratios)\n",
    "    if not correction_df.empty:\n",
    "        correction_df.plot.scatter(x='correction_ratio', y='relative_engagement', \n",
    "                                 s=correction_df['subreddit_overlap_ratio'] * 100 + 50,\n",
    "                                 alpha=0.6, ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Correction Effectiveness')\n",
    "        axes[0, 1].set_xlabel('Correction Ratio (Corrections/Claims)')\n",
    "        axes[0, 1].set_ylabel('Relative Engagement (Correction/Claim Score)')\n",
    "        axes[0, 1].grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 3: Top information sources and receivers\n",
    "    source_df = pd.DataFrame({\n",
    "        'Subreddit': [s[0] for s in network_metrics['top_sources']],\n",
    "        'Out Degree Centrality': [s[1] for s in network_metrics['top_sources']]\n",
    "    })\n",
    "    receiver_df = pd.DataFrame({\n",
    "        'Subreddit': [r[0] for r in network_metrics['top_receivers']],\n",
    "        'In Degree Centrality': [r[1] for r in network_metrics['top_receivers']]\n",
    "    })\n",
    "    \n",
    "    source_df.set_index('Subreddit').plot(kind='bar', ax=axes[1, 0], color='coral')\n",
    "    axes[1, 0].set_title('Top Information Sources')\n",
    "    axes[1, 0].set_ylabel('Out Degree Centrality')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    receiver_df.set_index('Subreddit').plot(kind='bar', ax=axes[1, 1], color='lightgreen')\n",
    "    axes[1, 1].set_title('Top Information Receivers')\n",
    "    axes[1, 1].set_ylabel('In Degree Centrality')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Misinformation Spread Analysis Dashboard', fontsize=20, y=1.02)\n",
    "    plt.savefig('misinformation_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def generate_insights_report(claims_df, claim_spread_df, corrections_df, propagation_metrics, network_metrics, linguistic_metrics):\n",
    "    \"\"\"\n",
    "    Generate a summary report of key insights\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    claims_df : pandas.DataFrame\n",
    "        DataFrame containing identified claims\n",
    "    claim_spread_df : pandas.DataFrame\n",
    "        DataFrame containing information about claim spread\n",
    "    corrections_df : pandas.DataFrame\n",
    "        DataFrame containing identified corrections\n",
    "    propagation_metrics : dict\n",
    "        Dictionary containing metrics about correction propagation\n",
    "    network_metrics : dict\n",
    "        Dictionary containing network analysis metrics\n",
    "    linguistic_metrics : dict\n",
    "        Dictionary containing metrics about linguistic differences\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    insights : dict\n",
    "        Dictionary containing key insights\n",
    "    \"\"\"\n",
    "    insights = {\n",
    "        'num_claims': len(claims_df),\n",
    "        'num_posts': len(claim_spread_df),\n",
    "        'num_corrections': len(corrections_df),\n",
    "        'communities_analyzed': len(claim_spread_df['subreddit'].unique()),\n",
    "        'key_findings': []\n",
    "    }\n",
    "    \n",
    "    # Calculate average time to first correction\n",
    "    correction_times = [metrics['time_to_first_correction'] for _, metrics in propagation_metrics.items() \n",
    "                       if metrics['time_to_first_correction'] is not None]\n",
    "    if correction_times:\n",
    "        avg_correction_time = sum(correction_times) / len(correction_times)\n",
    "        insights['avg_time_to_correction'] = avg_correction_time\n",
    "        \n",
    "        insights['key_findings'].append(\n",
    "            f\"On average, it takes {avg_correction_time:.1f} hours for the first correction to appear after a claim is posted.\"\n",
    "        )\n",
    "    \n",
    "    # Find subreddits most/least susceptible to corrections\n",
    "    subreddit_correction_ratios = {}\n",
    "    for claim_id, metrics in propagation_metrics.items():\n",
    "        for subreddit in metrics['claim_subreddits']:\n",
    "            if subreddit not in subreddit_correction_ratios:\n",
    "                subreddit_correction_ratios[subreddit] = []\n",
    "            \n",
    "            # Check if this subreddit received corrections for this claim\n",
    "            if subreddit in metrics['correction_subreddits']:\n",
    "                subreddit_correction_ratios[subreddit].append(1)\n",
    "            else:\n",
    "                subreddit_correction_ratios[subreddit].append(0)\n",
    "    \n",
    "    # Calculate average correction ratios\n",
    "    avg_correction_ratios = {\n",
    "        subreddit: sum(ratios) / len(ratios) \n",
    "        for subreddit, ratios in subreddit_correction_ratios.items() \n",
    "        if len(ratios) >= 3  # Only consider subreddits with at least 3 claims\n",
    "    }\n",
    "    \n",
    "    if avg_correction_ratios:\n",
    "        # Most and least susceptible subreddits\n",
    "        most_susceptible = max(avg_correction_ratios.items(), key=lambda x: x[1])\n",
    "        least_susceptible = min(avg_correction_ratios.items(), key=lambda x: x[1])\n",
    "        \n",
    "        insights['most_susceptible_subreddit'] = most_susceptible[0]\n",
    "        insights['least_susceptible_subreddit'] = least_susceptible[0]\n",
    "        \n",
    "        insights['key_findings'].append(\n",
    "            f\"r/{most_susceptible[0]} is most susceptible to corrections, with {most_susceptible[1]*100:.1f}% of claims receiving corrections.\"\n",
    "        )\n",
    "        insights['key_findings'].append(\n",
    "            f\"r/{least_susceptible[0]} is least susceptible to corrections, with only {least_susceptible[1]*100:.1f}% of claims receiving corrections.\"\n",
    "        )\n",
    "    \n",
    "    # Find top information bridges\n",
    "    if network_metrics.get('top_bridges'):\n",
    "        top_bridge = network_metrics['top_bridges'][0]\n",
    "        insights['top_information_bridge'] = top_bridge[0]\n",
    "        \n",
    "        insights['key_findings'].append(\n",
    "            f\"r/{top_bridge[0]} serves as the most important bridge for information flow between different political communities.\"\n",
    "        )\n",
    "    \n",
    "    # Analyze linguistic differences\n",
    "    sentiment_by_subreddit = {}\n",
    "    for claim_id, subreddit_data in linguistic_metrics.items():\n",
    "        for subreddit, metrics in subreddit_data.items():\n",
    "            if subreddit not in sentiment_by_subreddit:\n",
    "                sentiment_by_subreddit[subreddit] = []\n",
    "            sentiment_by_subreddit[subreddit].append(metrics['avg_sentiment'])\n",
    "    \n",
    "    # Calculate average sentiment\n",
    "    avg_sentiments = {\n",
    "        subreddit: sum(sentiments) / len(sentiments)\n",
    "        for subreddit, sentiments in sentiment_by_subreddit.items()\n",
    "        if len(sentiments) >= 3  # Only consider subreddits with at least 3 claims\n",
    "    }\n",
    "    \n",
    "    if avg_sentiments:\n",
    "        # Most positive and negative subreddits\n",
    "        most_positive = max(avg_sentiments.items(), key=lambda x: x[1])\n",
    "        most_negative = min(avg_sentiments.items(), key=lambda x: x[1])\n",
    "        \n",
    "        insights['most_positive_subreddit'] = most_positive[0]\n",
    "        insights['most_negative_subreddit'] = most_negative[0]\n",
    "        \n",
    "        insights['key_findings'].append(\n",
    "            f\"r/{most_positive[0]} presents information with the most positive sentiment (avg: {most_positive[1]:.2f}).\"\n",
    "        )\n",
    "        insights['key_findings'].append(\n",
    "            f\"r/{most_negative[0]} presents information with the most negative sentiment (avg: {most_negative[1]:.2f}).\"\n",
    "        )\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Example usage\n",
    "\"\"\"\n",
    "# Create summary dashboard\n",
    "create_summary_dashboard(claim_spread_df, corrections_df, propagation_metrics, network_metrics)\n",
    "\n",
    "# Generate insights report\n",
    "insights = generate_insights_report(claims_df, claim_spread_df, corrections_df, \n",
    "                                  propagation_metrics, network_metrics, linguistic_metrics)\n",
    "\n",
    "# Save insights\n",
    "with open('misinformation_insights.json', 'w') as f:\n",
    "    json.dump(insights, f, indent=2)\n",
    "\n",
    "# Print key findings\n",
    "print(\"Key Findings:\")\n",
    "for idx, finding in enumerate(insights['key_findings'], 1):\n",
    "    print(f\"{idx}. {finding}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7296047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c802ac1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Create information flow network\\nG = create_information_flow_network(claim_spread_df)\\n\\n# Analyze network\\nnetwork_metrics = analyze_network(G)\\n\\n# Visualize network\\nvisualize_information_flow(G, network_metrics)\\n\\n# Save network data\\nwith open('information_flow_network.pkl', 'wb') as f:\\n    pickle.dump({'graph': G, 'metrics': network_metrics}, f)\\n\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================\n",
    "# Part 5: Network Analysis of Information Flow\n",
    "# =============================================\n",
    "\n",
    "def create_information_flow_network(claim_spread_df):\n",
    "    \"\"\"\n",
    "    Create a network representation of information flow between subreddits\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    claim_spread_df : pandas.DataFrame\n",
    "        DataFrame containing information about claim spread\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    G : networkx.DiGraph\n",
    "        Directed graph representing information flow\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Group by claim_id\n",
    "    for claim_id, claim_data in claim_spread_df.groupby('claim_id'):\n",
    "        # Sort by creation time\n",
    "        claim_data = claim_data.sort_values('created_utc')\n",
    "        \n",
    "        # Add nodes (subreddits)\n",
    "        for subreddit in claim_data['subreddit'].unique():\n",
    "            if subreddit not in G:\n",
    "                G.add_node(subreddit, type='subreddit')\n",
    "        \n",
    "        # Add edges based on temporal flow\n",
    "        subreddits_seen = []\n",
    "        for _, post in claim_data.iterrows():\n",
    "            subreddit = post['subreddit']\n",
    "            \n",
    "            # Add edges from all previously seen subreddits to current subreddit\n",
    "            for prev_subreddit in subreddits_seen:\n",
    "                if prev_subreddit != subreddit:\n",
    "                    # Check if edge already exists\n",
    "                    if G.has_edge(prev_subreddit, subreddit):\n",
    "                        # Increment weight\n",
    "                        G[prev_subreddit][subreddit]['weight'] += 1\n",
    "                    else:\n",
    "                        # Create new edge\n",
    "                        G.add_edge(prev_subreddit, subreddit, weight=1, claims=[claim_id])\n",
    "            \n",
    "            # Add current subreddit to seen list if not already there\n",
    "            if subreddit not in subreddits_seen:\n",
    "                subreddits_seen.append(subreddit)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def analyze_network(G):\n",
    "    \"\"\"\n",
    "    Analyze the information flow network\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    G : networkx.DiGraph\n",
    "        Directed graph representing information flow\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    network_metrics : dict\n",
    "        Dictionary containing network analysis metrics\n",
    "    \"\"\"\n",
    "    # Calculate basic network metrics\n",
    "    metrics = {\n",
    "        'num_nodes': G.number_of_nodes(),\n",
    "        'num_edges': G.number_of_edges(),\n",
    "        'density': nx.density(G),\n",
    "        'is_strongly_connected': nx.is_strongly_connected(G)\n",
    "    }\n",
    "    \n",
    "    # Centrality measures\n",
    "    metrics['out_degree_centrality'] = nx.out_degree_centrality(G)\n",
    "    metrics['in_degree_centrality'] = nx.in_degree_centrality(G)\n",
    "    metrics['betweenness_centrality'] = nx.betweenness_centrality(G)\n",
    "    \n",
    "    # Community detection\n",
    "    # Convert to undirected for community detection\n",
    "    G_undirected = G.to_undirected()\n",
    "    partition = community_louvain.best_partition(G_undirected)\n",
    "    metrics['communities'] = partition\n",
    "    \n",
    "    # Identify key nodes\n",
    "    sorted_out_degree = sorted(metrics['out_degree_centrality'].items(), \n",
    "                              key=lambda x: x[1], reverse=True)\n",
    "    sorted_in_degree = sorted(metrics['in_degree_centrality'].items(), \n",
    "                             key=lambda x: x[1], reverse=True)\n",
    "    sorted_betweenness = sorted(metrics['betweenness_centrality'].items(), \n",
    "                               key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    metrics['top_sources'] = sorted_out_degree[:5]  # Top 5 sources of information\n",
    "    metrics['top_receivers'] = sorted_in_degree[:5]  # Top 5 receivers of information\n",
    "    metrics['top_bridges'] = sorted_betweenness[:5]  # Top 5 information bridges\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def visualize_information_flow(G, metrics, output_file='information_flow_network.png'):\n",
    "    \"\"\"\n",
    "    Visualize the information flow network\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    G : networkx.DiGraph\n",
    "        Directed graph representing information flow\n",
    "    metrics : dict\n",
    "        Dictionary containing network analysis metrics\n",
    "    output_file : str\n",
    "        Path to save the visualization\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Set up node positions using spring layout\n",
    "    pos = nx.spring_layout(G, k=0.3, iterations=50)\n",
    "    \n",
    "    # Get node attributes for visualization\n",
    "    communities = metrics['communities']\n",
    "    node_colors = [communities[node] for node in G.nodes()]\n",
    "    \n",
    "    # Scale node sizes based on betweenness centrality\n",
    "    betweenness = metrics['betweenness_centrality']\n",
    "    node_sizes = [betweenness[node] * 5000 + 100 for node in G.nodes()]\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, \n",
    "                          node_color=node_colors, \n",
    "                          node_size=node_sizes,\n",
    "                          alpha=0.8, \n",
    "                          cmap=plt.cm.tab20)\n",
    "    \n",
    "    # Draw edges with weights as width\n",
    "    edge_weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "    nx.draw_networkx_edges(G, pos, \n",
    "                          width=[w * 0.5 for w in edge_weights], \n",
    "                          alpha=0.5, \n",
    "                          edge_color='gray',\n",
    "                          arrowsize=15)\n",
    "    \n",
    "    # Draw node labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')\n",
    "    \n",
    "    plt.title('Information Flow Network Between Political Subreddits', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "\"\"\"\n",
    "# Create information flow network\n",
    "G = create_information_flow_network(claim_spread_df)\n",
    "\n",
    "# Analyze network\n",
    "network_metrics = analyze_network(G)\n",
    "\n",
    "# Visualize network\n",
    "visualize_information_flow(G, network_metrics)\n",
    "\n",
    "# Save network data\n",
    "with open('information_flow_network.pkl', 'wb') as f:\n",
    "    pickle.dump({'graph': G, 'metrics': network_metrics}, f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "46744b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Identify corrections\\ncorrections_df = identify_corrections(posts_df, comments_df, claim_spread_df)\\n\\n# Analyze correction propagation\\npropagation_metrics = analyze_correction_propagation(claim_spread_df, corrections_df)\\n\\n# Visualize correction propagation for a specific claim\\nclaim_id = claim_spread_df['claim_id'].iloc[0]  # Just get the first claim for example\\nvisualize_correction_propagation(claim_spread_df, corrections_df, claim_id)\\n\\n# Save correction data\\ncorrections_df.to_csv('corrections.csv', index=False)\\nwith open('propagation_metrics.json', 'w') as f:\\n    json.dump(propagation_metrics, f, default=str)  # default=str to handle datetime objects\\n\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================\n",
    "# Part 6: Analyze Correction Propagation\n",
    "# =============================================\n",
    "\n",
    "def identify_corrections(posts_df, comments_df, claim_spread_df, similarity_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Identify posts and comments that might be corrections to misinformation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    posts_df : pandas.DataFrame\n",
    "        DataFrame containing all posts\n",
    "    comments_df : pandas.DataFrame\n",
    "        DataFrame containing all comments\n",
    "    claim_spread_df : pandas.DataFrame\n",
    "        DataFrame containing information about claim spread\n",
    "    similarity_threshold : float\n",
    "        Threshold for similarity to consider a post/comment as related to a claim\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    corrections_df : pandas.DataFrame\n",
    "        DataFrame containing identified corrections\n",
    "    \"\"\"\n",
    "    # Keywords that might indicate a correction\n",
    "    correction_keywords = [\n",
    "        'correction', 'false', 'debunk', 'fact check', 'misleading', 'incorrect',\n",
    "        'wrong', 'not true', 'misinformation', 'fake news', 'disinformation'\n",
    "    ]\n",
    "    \n",
    "    # Create a regex pattern for the keywords\n",
    "    pattern = '|'.join(correction_keywords)\n",
    "    \n",
    "    # Find potential corrections in posts\n",
    "    post_corrections = []\n",
    "    for claim_id in claim_spread_df['claim_id'].unique():\n",
    "        # Get posts related to this claim\n",
    "        claim_posts = claim_spread_df[claim_spread_df['claim_id'] == claim_id]\n",
    "        claim_post_ids = claim_posts['post_id'].tolist()\n",
    "        \n",
    "        # Find posts that might be corrections\n",
    "        for _, post in posts_df.iterrows():\n",
    "            # Skip posts that are already part of the claim spread\n",
    "            if post['post_id'] in claim_post_ids:\n",
    "                continue\n",
    "                \n",
    "            # Check if post contains correction keywords\n",
    "            title_text = str(post.get('title', ''))\n",
    "            body_text = str(post.get('body', ''))\n",
    "            combined_text = title_text + ' ' + body_text\n",
    "            \n",
    "            if re.search(pattern, combined_text, re.IGNORECASE):\n",
    "                # Calculate similarity to claim posts\n",
    "                claim_texts = []\n",
    "                for pid in claim_post_ids:\n",
    "                    claim_post = posts_df[posts_df['post_id'] == pid]\n",
    "                    if not claim_post.empty and 'combined_text' in claim_post.columns:\n",
    "                        text = claim_post['combined_text'].iloc[0]\n",
    "                        if isinstance(text, str) and text.strip():\n",
    "                            claim_texts.append(text)\n",
    "                \n",
    "                post_text = post.get('combined_text', '')\n",
    "                \n",
    "                # Skip if post text is empty\n",
    "                if not isinstance(post_text, str) or not post_text.strip():\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate similarities\n",
    "                similarities = []\n",
    "                for claim_text in claim_texts:\n",
    "                    if not claim_text.strip():\n",
    "                        continue\n",
    "                    try:\n",
    "                        # Use simple TF-IDF vectorizer for this comparison\n",
    "                        tfidf = TfidfVectorizer().fit_transform([post_text, claim_text])\n",
    "                        similarity = cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]\n",
    "                        similarities.append(similarity)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating similarity: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # If any similarity is above threshold, consider it a correction\n",
    "                if similarities and max(similarities) > similarity_threshold:\n",
    "                    correction = {\n",
    "                        'claim_id': claim_id,\n",
    "                        'correction_type': 'post',\n",
    "                        'correction_id': post['post_id'],\n",
    "                        'subreddit': post.get('subreddit', ''),\n",
    "                        'author': post.get('author', ''),\n",
    "                        'created_utc': post.get('created_utc'),\n",
    "                        'score': post.get('score', 0),\n",
    "                        'sentiment': post.get('sentiment', 0),\n",
    "                        'max_similarity': max(similarities) if similarities else 0\n",
    "                    }\n",
    "                    post_corrections.append(correction)\n",
    "    \n",
    "    # Find potential corrections in comments\n",
    "    comment_corrections = []\n",
    "    for claim_id in claim_spread_df['claim_id'].unique():\n",
    "        # Get posts related to this claim\n",
    "        claim_posts = claim_spread_df[claim_spread_df['claim_id'] == claim_id]\n",
    "        claim_post_ids = claim_posts['post_id'].tolist()\n",
    "        \n",
    "        # Get comments for these posts\n",
    "        if 'post_id' in comments_df.columns:\n",
    "            related_comments = comments_df[comments_df['post_id'].isin(claim_post_ids)]\n",
    "        else:\n",
    "            related_comments = pd.DataFrame()  # Empty DataFrame if post_id not in comments\n",
    "        \n",
    "        # Find comments that might be corrections\n",
    "        for _, comment in related_comments.iterrows():\n",
    "            # Check if comment contains correction keywords\n",
    "            comment_body = str(comment.get('body', ''))\n",
    "            if re.search(pattern, comment_body, re.IGNORECASE):\n",
    "                correction = {\n",
    "                    'claim_id': claim_id,\n",
    "                    'correction_type': 'comment',\n",
    "                    'correction_id': comment.get('comment_id', ''),\n",
    "                    'post_id': comment.get('post_id', ''),\n",
    "                    'subreddit': comment.get('subreddit', ''),\n",
    "                    'author': comment.get('author', ''),\n",
    "                    'created_utc': comment.get('created_utc'),\n",
    "                    'score': comment.get('score', 0),\n",
    "                    'sentiment': comment.get('sentiment', 0)\n",
    "                }\n",
    "                comment_corrections.append(correction)\n",
    "    \n",
    "    # Combine post and comment corrections\n",
    "    all_corrections = post_corrections + comment_corrections\n",
    "    \n",
    "    if all_corrections:\n",
    "        corrections_df = pd.DataFrame(all_corrections)\n",
    "    else:\n",
    "        # Create empty DataFrame with correct columns if no corrections found\n",
    "        corrections_df = pd.DataFrame(columns=[\n",
    "            'claim_id', 'correction_type', 'correction_id', 'post_id', \n",
    "            'subreddit', 'author', 'created_utc', 'score', 'sentiment', 'max_similarity'\n",
    "        ])\n",
    "    \n",
    "    return corrections_df\n",
    "\n",
    "def analyze_correction_propagation(claim_spread_df, corrections_df):\n",
    "    \"\"\"\n",
    "    Analyze how corrections propagate compared to original claims\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    claim_spread_df : pandas.DataFrame\n",
    "        DataFrame containing information about claim spread\n",
    "    corrections_df : pandas.DataFrame\n",
    "        DataFrame containing identified corrections\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    propagation_metrics : dict\n",
    "        Dictionary containing metrics about correction propagation\n",
    "    \"\"\"\n",
    "    propagation_metrics = {}\n",
    "    \n",
    "    # For each claim\n",
    "    for claim_id in claim_spread_df['claim_id'].unique():\n",
    "        # Get claim spread data\n",
    "        claim_data = claim_spread_df[claim_spread_df['claim_id'] == claim_id]\n",
    "        \n",
    "        # Get corrections for this claim\n",
    "        claim_corrections = corrections_df[corrections_df['claim_id'] == claim_id]\n",
    "        \n",
    "        # Calculate time metrics\n",
    "        if 'created_utc' in claim_data.columns and len(claim_data) > 0:\n",
    "            claim_first_appearance = claim_data['created_utc'].min()\n",
    "            \n",
    "            if not claim_corrections.empty and 'created_utc' in claim_corrections.columns:\n",
    "                first_correction = claim_corrections['created_utc'].min()\n",
    "                # Calculate time difference in hours\n",
    "                try:\n",
    "                    time_to_first_correction = (first_correction - claim_first_appearance).total_seconds() / 3600\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating time difference: {e}\")\n",
    "                    time_to_first_correction = None\n",
    "            else:\n",
    "                first_correction = None\n",
    "                time_to_first_correction = None\n",
    "        else:\n",
    "            claim_first_appearance = None\n",
    "            first_correction = None\n",
    "            time_to_first_correction = None\n",
    "        \n",
    "        # Calculate subreddit overlap\n",
    "        if 'subreddit' in claim_data.columns and len(claim_data) > 0:\n",
    "            claim_subreddits = set(claim_data['subreddit'])\n",
    "            \n",
    "            if not claim_corrections.empty and 'subreddit' in claim_corrections.columns:\n",
    "                correction_subreddits = set(claim_corrections['subreddit'])\n",
    "                subreddit_overlap = len(claim_subreddits.intersection(correction_subreddits))\n",
    "                subreddit_overlap_ratio = subreddit_overlap / len(claim_subreddits) if claim_subreddits else 0\n",
    "            else:\n",
    "                correction_subreddits = set()\n",
    "                subreddit_overlap = 0\n",
    "                subreddit_overlap_ratio = 0\n",
    "        else:\n",
    "            claim_subreddits = set()\n",
    "            correction_subreddits = set()\n",
    "            subreddit_overlap = 0\n",
    "            subreddit_overlap_ratio = 0\n",
    "        \n",
    "        # Calculate engagement metrics\n",
    "        if 'score' in claim_data.columns and len(claim_data) > 0:\n",
    "            avg_claim_score = claim_data['score'].mean()\n",
    "            \n",
    "            if not claim_corrections.empty and 'score' in claim_corrections.columns:\n",
    "                avg_correction_score = claim_corrections['score'].mean()\n",
    "                relative_engagement = avg_correction_score / avg_claim_score if avg_claim_score != 0 else 0\n",
    "            else:\n",
    "                avg_correction_score = 0\n",
    "                relative_engagement = 0\n",
    "        else:\n",
    "            avg_claim_score = 0\n",
    "            avg_correction_score = 0\n",
    "            relative_engagement = 0\n",
    "        \n",
    "        # Store metrics\n",
    "        propagation_metrics[claim_id] = {\n",
    "            'claim_first_appearance': claim_first_appearance,\n",
    "            'first_correction': first_correction,\n",
    "            'time_to_first_correction': time_to_first_correction,\n",
    "            'num_claim_posts': len(claim_data),\n",
    "            'num_corrections': len(claim_corrections),\n",
    "            'correction_ratio': len(claim_corrections) / len(claim_data) if len(claim_data) > 0 else 0,\n",
    "            'claim_subreddits': list(claim_subreddits),  # Convert to list for JSON serialization\n",
    "            'correction_subreddits': list(correction_subreddits),  # Convert to list for JSON serialization\n",
    "            'subreddit_overlap': subreddit_overlap,\n",
    "            'subreddit_overlap_ratio': subreddit_overlap_ratio,\n",
    "            'avg_claim_score': avg_claim_score,\n",
    "            'avg_correction_score': avg_correction_score,\n",
    "            'relative_engagement': relative_engagement\n",
    "        }\n",
    "    \n",
    "    return propagation_metrics\n",
    "\n",
    "def visualize_correction_propagation(claim_spread_df, corrections_df, claim_id):\n",
    "    \"\"\"\n",
    "    Visualize how a specific claim and its corrections propagate over time\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    claim_spread_df : pandas.DataFrame\n",
    "        DataFrame containing information about claim spread\n",
    "    corrections_df : pandas.DataFrame\n",
    "        DataFrame containing identified corrections\n",
    "    claim_id : str\n",
    "        ID of the claim to visualize\n",
    "    \"\"\"\n",
    "    # Get data for this claim\n",
    "    claim_data = claim_spread_df[claim_spread_df['claim_id'] == claim_id]\n",
    "    claim_corrections = corrections_df[corrections_df['claim_id'] == claim_id]\n",
    "    \n",
    "    # Skip if no data\n",
    "    if claim_data.empty:\n",
    "        print(f\"No claim data available for claim {claim_id}\")\n",
    "        return\n",
    "    \n",
    "    # Create a timeline\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot claim posts\n",
    "    for idx, (_, post) in enumerate(claim_data.iterrows()):\n",
    "        # Check if score is available\n",
    "        score = post.get('score', 50)  # Default value if score is missing\n",
    "        if not isinstance(score, (int, float)):\n",
    "            score = 50\n",
    "        \n",
    "        # Ensure created_utc is datetime\n",
    "        if not isinstance(post.get('created_utc'), pd.Timestamp):\n",
    "            continue\n",
    "            \n",
    "        plt.scatter(post['created_utc'], idx % 10, c='red', s=score/10 + 50, alpha=0.7)\n",
    "        plt.text(post['created_utc'], idx % 10 + 0.5, post.get('subreddit', 'unknown'), fontsize=8)\n",
    "    \n",
    "    # Plot corrections if any exist\n",
    "    if not claim_corrections.empty:\n",
    "        for idx, (_, correction) in enumerate(claim_corrections.iterrows()):\n",
    "            # Check if score is available\n",
    "            score = correction.get('score', 50)  # Default value if score is missing\n",
    "            if not isinstance(score, (int, float)):\n",
    "                score = 50\n",
    "            \n",
    "            # Ensure created_utc is datetime\n",
    "            if not isinstance(correction.get('created_utc'), pd.Timestamp):\n",
    "                continue\n",
    "                \n",
    "            plt.scatter(correction['created_utc'], (idx % 10) - 15, c='green', s=score/10 + 50, alpha=0.7)\n",
    "            plt.text(correction['created_utc'], (idx % 10) - 14.5, correction.get('subreddit', 'unknown'), fontsize=8)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Position (for visualization only)')\n",
    "    plt.title(f'Propagation Timeline for Claim {claim_id}')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.scatter([], [], c='red', s=100, label='Original claim posts')\n",
    "    plt.scatter([], [], c='green', s=100, label='Correction posts/comments')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Format x-axis as dates\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "\"\"\"\n",
    "# Identify corrections\n",
    "corrections_df = identify_corrections(posts_df, comments_df, claim_spread_df)\n",
    "\n",
    "# Analyze correction propagation\n",
    "propagation_metrics = analyze_correction_propagation(claim_spread_df, corrections_df)\n",
    "\n",
    "# Visualize correction propagation for a specific claim\n",
    "claim_id = claim_spread_df['claim_id'].iloc[0]  # Just get the first claim for example\n",
    "visualize_correction_propagation(claim_spread_df, corrections_df, claim_id)\n",
    "\n",
    "# Save correction data\n",
    "corrections_df.to_csv('corrections.csv', index=False)\n",
    "with open('propagation_metrics.json', 'w') as f:\n",
    "    json.dump(propagation_metrics, f, default=str)  # default=str to handle datetime objects\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2a3ae3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(posts_csv_path, comments_csv_path):\n",
    "    \"\"\"\n",
    "    Execute the full misinformation tracking pipeline using existing CSV files\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    posts_csv_path : str\n",
    "        Path to the CSV file containing Reddit posts\n",
    "    comments_csv_path : str\n",
    "        Path to the CSV file containing Reddit comments\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing all analysis results\n",
    "    \"\"\"\n",
    "    print(\"Starting misinformation tracking pipeline with existing CSV data...\")\n",
    "    \n",
    "    # Step 1: Load Data from CSV\n",
    "    print(\"\\n1. Loading data from CSV files...\")\n",
    "    # Try to handle potential encoding issues\n",
    "    try:\n",
    "        posts_df = pd.read_csv(posts_csv_path)\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"UTF-8 encoding failed for posts, trying latin-1...\")\n",
    "        posts_df = pd.read_csv(posts_csv_path, encoding='latin-1')\n",
    "    \n",
    "    try:\n",
    "        comments_df = pd.read_csv(comments_csv_path)\n",
    "    except UnicodeDecodeError:\n",
    "        print(\"UTF-8 encoding failed for comments, trying latin-1...\")\n",
    "        comments_df = pd.read_csv(comments_csv_path, encoding='latin-1')\n",
    "    \n",
    "    # Print data summary\n",
    "    print(f\"\\nLoaded {len(posts_df)} posts from {posts_df['subreddit'].nunique()} subreddits.\")\n",
    "    print(f\"Loaded {len(comments_df)} comments.\")\n",
    "    \n",
    "    print(\"\\nSubreddits in the dataset:\")\n",
    "    for subreddit, count in posts_df['subreddit'].value_counts().items():\n",
    "        print(f\"  - r/{subreddit}: {count} posts\")\n",
    "    \n",
    "    # Step 2: Data Preprocessing\n",
    "    processed_posts_path = 'processed_posts.csv'\n",
    "    processed_comments_path = 'processed_comments.csv'\n",
    "    \n",
    "    # Check if processed files already exist\n",
    "    import os\n",
    "    if os.path.exists(processed_posts_path) and os.path.exists(processed_comments_path):\n",
    "        print(\"\\n2. Loading existing processed data files...\")\n",
    "        try:\n",
    "            posts_df = pd.read_csv(processed_posts_path)\n",
    "            comments_df = pd.read_csv(processed_comments_path)\n",
    "            \n",
    "            # Convert timestamp columns to datetime\n",
    "            if 'created_utc' in posts_df.columns:\n",
    "                posts_df['created_utc'] = pd.to_datetime(posts_df['created_utc'])\n",
    "            if 'created_utc' in comments_df.columns:\n",
    "                comments_df['created_utc'] = pd.to_datetime(comments_df['created_utc'])\n",
    "            \n",
    "            # Convert entities column from string back to list if needed\n",
    "            if 'entities' in posts_df.columns and posts_df['entities'].dtype == 'object':\n",
    "                try:\n",
    "                    import ast\n",
    "                    posts_df['entities'] = posts_df['entities'].apply(\n",
    "                        lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "                    )\n",
    "                except:\n",
    "                    print(\"Warning: Could not convert entities column back to list.\")\n",
    "            \n",
    "            if 'entities' in comments_df.columns and comments_df['entities'].dtype == 'object':\n",
    "                try:\n",
    "                    import ast\n",
    "                    comments_df['entities'] = comments_df['entities'].apply(\n",
    "                        lambda x: ast.literal_eval(x) if isinstance(x, str) else []\n",
    "                    )\n",
    "                except:\n",
    "                    print(\"Warning: Could not convert entities column back to list.\")\n",
    "            \n",
    "            print(f\"Loaded processed data: {len(posts_df)} posts and {len(comments_df)} comments.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading processed files: {e}\")\n",
    "            print(\"Will preprocess data from scratch...\")\n",
    "            posts_df, comments_df = process_dataframes(posts_df, comments_df)\n",
    "            \n",
    "            # Save processed data\n",
    "            posts_df.to_csv(processed_posts_path, index=False)\n",
    "            comments_df.to_csv(processed_comments_path, index=False)\n",
    "            print(f\"Saved processed data to {processed_posts_path} and {processed_comments_path}\")\n",
    "    else:\n",
    "        print(\"\\n2. Processing and enriching data...\")\n",
    "        posts_df, comments_df = process_dataframes(posts_df, comments_df)\n",
    "        \n",
    "        # Save processed data\n",
    "        posts_df.to_csv(processed_posts_path, index=False)\n",
    "        comments_df.to_csv(processed_comments_path, index=False)\n",
    "        print(f\"Saved processed data to {processed_posts_path} and {processed_comments_path}\")\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        'posts_df': posts_df,\n",
    "        'comments_df': comments_df\n",
    "    }\n",
    "    \n",
    "    # Step 3: Topic Modeling (skip if too few posts)\n",
    "    print(\"\\n3. Creating topic model to identify claims/stories...\")\n",
    "    lda_model, corpus, dictionary = create_topic_model(posts_df, num_topics=15, passes=20, min_posts=5)\n",
    "    \n",
    "    # If topic modeling failed, return early with just the processed data\n",
    "    if lda_model is None:\n",
    "        print(\"Topic modeling failed. Returning processed data only.\")\n",
    "        return results\n",
    "    \n",
    "    # Display topics\n",
    "    print(\"\\nIdentified Topics:\")\n",
    "    for idx, topic in lda_model.print_topics(num_words=10):\n",
    "        print(f\"Topic {idx}: {topic}\")\n",
    "    \n",
    "    # Get topic distributions\n",
    "    posts_df = get_topic_distributions(lda_model, corpus, posts_df)\n",
    "    \n",
    "    # Save model and data\n",
    "    lda_model.save('lda_model.model')\n",
    "    with open('lda_corpus.pkl', 'wb') as f:\n",
    "        pickle.dump(corpus, f)\n",
    "    dictionary.save('lda_dictionary.dict')\n",
    "    \n",
    "    # Update results\n",
    "    results['posts_df'] = posts_df\n",
    "    results['lda_model'] = lda_model\n",
    "    results['corpus'] = corpus\n",
    "    results['dictionary'] = dictionary\n",
    "    \n",
    "    # Step 4: Claim Identification and Tracking\n",
    "    print(\"\\n4. Identifying potential claims...\")\n",
    "    claims_df = identify_potential_claims(posts_df, threshold=0.6)\n",
    "    \n",
    "    # If no claims were identified, return early\n",
    "    if claims_df.empty:\n",
    "        print(\"No claims identified. Returning processed data and topic model only.\")\n",
    "        results['claims_df'] = claims_df\n",
    "        return results\n",
    "    \n",
    "    print(\"\\n5. Building similarity matrix...\")\n",
    "    similarity_matrix, post_ids, claim_ids = build_claim_similarity_matrix(posts_df, claims_df)\n",
    "    \n",
    "    print(\"\\n6. Tracking claim spread...\")\n",
    "    claim_spread_df = track_claim_spread(posts_df, comments_df, similarity_matrix, post_ids, claim_ids)\n",
    "    \n",
    "    # Save claim data\n",
    "    claims_df.to_csv('identified_claims.csv', index=False)\n",
    "    claim_spread_df.to_csv('claim_spread.csv', index=False)\n",
    "    \n",
    "    # Update results\n",
    "    results['claims_df'] = claims_df\n",
    "    results['claim_spread_df'] = claim_spread_df\n",
    "    \n",
    "    # Step 5: Network Analysis\n",
    "    print(\"\\n7. Creating information flow network...\")\n",
    "    G = create_information_flow_network(claim_spread_df)\n",
    "    \n",
    "    print(\"\\n8. Analyzing network...\")\n",
    "    network_metrics = analyze_network(G)\n",
    "    \n",
    "    print(\"\\n9. Visualizing information flow network...\")\n",
    "    visualize_information_flow(G, network_metrics)\n",
    "    \n",
    "    # Save network data\n",
    "    with open('information_flow_network.pkl', 'wb') as f:\n",
    "        pickle.dump({'graph': G, 'metrics': network_metrics}, f)\n",
    "    \n",
    "    # Update results\n",
    "    results['network_graph'] = G\n",
    "    results['network_metrics'] = network_metrics\n",
    "    \n",
    "    # Step 6: Correction Analysis\n",
    "    print(\"\\n10. Identifying corrections...\")\n",
    "    corrections_df = identify_corrections(posts_df, comments_df, claim_spread_df)\n",
    "    \n",
    "    print(\"\\n11. Analyzing correction propagation...\")\n",
    "    propagation_metrics = analyze_correction_propagation(claim_spread_df, corrections_df)\n",
    "    \n",
    "    # Skip visualization if no claims\n",
    "    if len(claim_spread_df) > 0:\n",
    "        print(\"\\n12. Visualizing correction propagation...\")\n",
    "        # Just visualize the first claim as an example\n",
    "        claim_id = claim_spread_df['claim_id'].iloc[0]\n",
    "        visualize_correction_propagation(claim_spread_df, corrections_df, claim_id)\n",
    "    \n",
    "    # Save correction data\n",
    "    corrections_df.to_csv('corrections.csv', index=False)\n",
    "    with open('propagation_metrics.json', 'w') as f:\n",
    "        json.dump(propagation_metrics, f, default=str)\n",
    "    \n",
    "    # Update results\n",
    "    results['corrections_df'] = corrections_df\n",
    "    results['propagation_metrics'] = propagation_metrics\n",
    "    \n",
    "    # Continue with remaining steps if there are claims...\n",
    "    # (linguistic analysis, summaries, etc.)\n",
    "    \n",
    "    print(\"\\nPipeline completed successfully!\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the processing pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ae0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting misinformation tracking pipeline with existing CSV data...\n",
      "\n",
      "1. Loading data from CSV files...\n",
      "\n",
      "Loaded 150505 posts from 3 subreddits.\n",
      "Loaded 3561862 comments.\n",
      "\n",
      "Subreddits in the dataset:\n",
      "  - r/Conservative: 132987 posts\n",
      "  - r/centrist: 11059 posts\n",
      "  - r/Liberal: 6459 posts\n",
      "\n",
      "2. Processing and enriching data...\n",
      "Processing posts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing post titles: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150505/150505 [01:18<00:00, 1907.44it/s]\n",
      "Processing post bodies: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150505/150505 [00:40<00:00, 3751.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing comments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing comments: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3561862/3561862 [38:26<00:00, 1544.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding sentiment analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating post sentiment: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150505/150505 [00:21<00:00, 7051.11it/s] \n",
      "Calculating comment sentiment:  10%|â–‰         | 343923/3561862 [66:17:39<13:15, 4046.63it/s]     "
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the pipeline\n",
    "results = main_pipeline(posts_csv_path, comments_csv_path)\n",
    "\n",
    "# You can access the processed dataframes\n",
    "processed_posts = results['posts_df']\n",
    "processed_comments = results['comments_df']\n",
    "\n",
    "# Display a sample of the processed data\n",
    "processed_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8fed738",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_posts_df = pd.read_csv(posts_csv_path)\n",
    "merged_comments_df = pd.read_csv(comments_csv_path)\n",
    "\n",
    "# I want to save head for each in csv\n",
    "merged_posts_df.head(10000).to_csv('merged_posts_head.csv', index=False)\n",
    "merged_comments_df.head(10000).to_csv('merged_comments_head.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebb0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7921a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e866e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f27337c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
